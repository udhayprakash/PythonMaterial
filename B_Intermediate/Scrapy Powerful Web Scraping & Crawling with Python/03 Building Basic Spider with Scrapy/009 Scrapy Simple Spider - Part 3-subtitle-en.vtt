WEBVTT

1
00:00:00.589 --> 00:00:04.259
Hey! So in this video we are going to cover this Scrapy

2
00:00:04.259 --> 00:00:08.357
code, or go over this code, that Scrapy by default wrote

3
00:00:08.357 --> 00:00:09.493
for us.

4
00:00:09.493 --> 00:00:13.567
And also we will go over the two data points that we scraped

5
00:00:13.567 --> 00:00:14.440
previously.

6
00:00:14.440 --> 00:00:15.711
Let's go to the site.

7
00:00:15.711 --> 00:00:18.747
So, this includes this data point and also this one

8
00:00:18.747 --> 00:00:22.708
and we will yield it in our Scrapy code somewhere

9
00:00:22.708 --> 00:00:23.737
in the parse function.

10
00:00:23.737 --> 00:00:26.717
So, let's begin with the source code itself.

11
00:00:26.717 --> 00:00:30.884
So, as you can see, the Spider subclasses scrapy.Spider

12
00:00:31.782 --> 00:00:34.868
and defines some attributes and methods.

13
00:00:34.868 --> 00:00:39.035
name, for example, identifies obviously the Spider name.

14
00:00:39.125 --> 00:00:42.453
It must be unique within the project, so you can't really

15
00:00:42.453 --> 00:00:46.489
set the same name for a different Spider even though

16
00:00:46.489 --> 00:00:50.377
they are in the same project structure.

17
00:00:50.377 --> 00:00:54.285
allowed_domains is a list of, obviously, allowed domains.

18
00:00:54.285 --> 00:00:57.895
If Scrapy encounters for example, some different domain

19
00:00:57.895 --> 00:01:02.062
other than quotes.toscrape.com it will not process it

20
00:01:03.209 --> 00:01:05.739
and it will automatically filter it out.

21
00:01:05.739 --> 00:01:09.041
Most of the time, you would encounter this if you are

22
00:01:09.041 --> 00:01:11.611
crawling every URL on the site.

23
00:01:11.611 --> 00:01:15.613
start_urls is either by default tuple or you

24
00:01:15.613 --> 00:01:17.738
can define it as a list.

25
00:01:17.738 --> 00:01:21.905
And it's just, it will be pretty much the first URL

26
00:01:23.022 --> 00:01:25.182
that Scrapy will process.

27
00:01:25.182 --> 00:01:29.349
You don't actually need to use www. in that part

28
00:01:29.833 --> 00:01:31.083
so let's exclude this.

29
00:01:31.083 --> 00:01:34.680
And parse is Scrapy default callback method

30
00:01:34.680 --> 00:01:38.846
in the scrapy.Spider or a.k.a. the basic template

31
00:01:40.552 --> 00:01:41.547
that Scrapy offers.

32
00:01:41.547 --> 00:01:45.714
So this method is called when or for the Request

33
00:01:46.333 --> 00:01:49.117
without an explicitly assigned callback.

34
00:01:49.117 --> 00:01:51.554
Defining some other name, for example, will not work

35
00:01:51.554 --> 00:01:55.119
so if you type here foobar, it will not work or

36
00:01:55.119 --> 00:01:57.202
Scrapy will get an error.

37
00:02:00.100 --> 00:02:02.638
It has self, obviously, and the response self is

38
00:02:02.638 --> 00:02:06.805
because it's in a class right and response because

39
00:02:07.080 --> 00:02:11.122
it will get the response object or HTML nodes

40
00:02:12.476 --> 00:02:16.059
or HTML source code from this page.

41
00:02:16.059 --> 00:02:20.226
Let's save it as it is and let's go back to the Terminal.

42
00:02:21.545 --> 00:02:24.102
And here we have two data points.

43
00:02:24.102 --> 00:02:27.335
So the first one is somewhere here above so the

44
00:02:27.335 --> 00:02:30.801
&lt;h1&gt; tag text which is located here.

45
00:02:31.229 --> 00:02:35.396
So if we copy this line and paste it into our parse method

46
00:02:38.062 --> 00:02:42.229
and assign something like h1_tag

47
00:02:42.743 --> 00:02:46.910
as a variable name and it's equal to this XPath selector

48
00:02:47.003 --> 00:02:51.004
and also let's go back to the Terminal.

49
00:02:51.004 --> 00:02:54.593
This is the second data point is located at the end

50
00:02:54.593 --> 00:02:58.759
so if we copy and paste this into our Scrapy code

51
00:03:00.290 --> 00:03:04.183
and name this, for example, tags variable is equal

52
00:03:04.183 --> 00:03:08.350
to pretty much this XPath selector and we will yield

53
00:03:08.470 --> 00:03:10.621
these two items.

54
00:03:10.621 --> 00:03:14.621
So by yielding pretty much these two data points

55
00:03:15.931 --> 00:03:20.098
we will print it out in Scrapy output, and then you

56
00:03:20.369 --> 00:03:24.536
can maybe get this data to the: CSV, JSON, XML or any other

57
00:03:26.758 --> 00:03:27.653
database.

58
00:03:27.653 --> 00:03:30.962
So to yield it we type in obviously the dictionary key.

59
00:03:30.962 --> 00:03:35.129
The first one will be &lt;h1&gt; tag, for example, and obviously

60
00:03:35.459 --> 00:03:38.418
the key will be &lt;h1&gt; tag.

61
00:03:39.721 --> 00:03:43.509
And then the second set of keys and values will be tags.

62
00:03:43.509 --> 00:03:47.676
So type in Tags as a key value and as an actual value,

63
00:03:50.504 --> 00:03:52.153
we type in tags.

64
00:03:52.153 --> 00:03:56.300
We will save this and go back to our Terminal.

65
00:03:56.300 --> 00:04:00.041
And open, actually, an additional Tab here.

66
00:04:01.145 --> 00:04:02.759
Let's maximize this.

67
00:04:02.759 --> 00:04:06.630
And, first things first, of course, we need to go

68
00:04:06.630 --> 00:04:08.438
to our root directory.

69
00:04:08.438 --> 00:04:12.214
So we are changing directory into the Desktop

70
00:04:12.214 --> 00:04:14.231
and then to the quotes_spider.

71
00:04:14.231 --> 00:04:18.398
We hit Enter and if configuration file is present, as it is,

72
00:04:18.841 --> 00:04:22.558
so we are in the root directory, which is great.

73
00:04:22.558 --> 00:04:24.576
So how do you actually run it or run

74
00:04:24.576 --> 00:04:28.743
the Scrapy spider code really is by typing in scrapy.

75
00:04:29.242 --> 00:04:33.409
You don't obviously have to do this, but command is here.

76
00:04:34.525 --> 00:04:37.310
So scrapy crawl, and then run a spider right.

77
00:04:37.310 --> 00:04:40.764
So pretty much type in scrapy, then crawl, and then

78
00:04:40.764 --> 00:04:42.051
name of the spider.

79
00:04:42.051 --> 00:04:45.161
Name of the spider can be retrieved either with

80
00:04:45.161 --> 00:04:48.103
the scrapy list, which will print out two spider names,

81
00:04:48.103 --> 00:04:51.110
so this is the one that we are looking for or you can go

82
00:04:51.110 --> 00:04:55.277
to your Text Editor and see that name is quotes.

83
00:04:55.371 --> 00:04:58.959
So let's copy it from here, go back to the Terminal.

84
00:04:58.959 --> 00:05:02.206
So type in scrapy, then crawl to run,

85
00:05:02.206 --> 00:05:06.373
then scrapy spider and then paste the Spider name.

86
00:05:07.576 --> 00:05:09.243
And let's hit Enter.

87
00:05:10.164 --> 00:05:14.331
And here we have a bunch of really output or logs

88
00:05:14.545 --> 00:05:18.712
and, as you can see, some things really are similar

89
00:05:19.843 --> 00:05:23.785
to the Scrapy Shell so that Scrapy is started overridden

90
00:05:23.785 --> 00:05:27.035
settings, extensions, middlewares, etc.

91
00:05:28.225 --> 00:05:31.646
What you need to know is just pretty much that

92
00:05:31.646 --> 00:05:35.386
when Scrapy is started and when it's finished really.

93
00:05:35.386 --> 00:05:39.553
So pretty much all of these data or rows of the output.

94
00:05:40.486 --> 00:05:43.954
So here we have that Spider is opened that it crawled

95
00:05:43.954 --> 00:05:46.953
at the beginning obviously zero pages, and you will get

96
00:05:46.953 --> 00:05:51.120
this exact message every minute or so, so you can get

97
00:05:52.277 --> 00:05:56.444
how many there are crawled page and also scraped page.

98
00:05:57.924 --> 00:06:02.091
Telnet console so you can copy and paste this IP address

99
00:06:02.098 --> 00:06:03.642
and then listen to it.

100
00:06:03.642 --> 00:06:06.976
And here we have the first URL which is scraped

101
00:06:06.976 --> 00:06:10.288
which is going to go to our home page and then go to

102
00:06:10.288 --> 00:06:12.600
the /robots.txt.

103
00:06:12.600 --> 00:06:16.767
This actually page is not present and that's the reason why

104
00:06:17.144 --> 00:06:18.656
we got 404.

105
00:06:18.656 --> 00:06:22.664
And then the second one is successful which just goes

106
00:06:22.664 --> 00:06:23.664
to the home page.

107
00:06:23.664 --> 00:06:27.170
So let's copy this link and then navigate to the

108
00:06:27.170 --> 00:06:30.390
Google Chrome and then paste it in here.

109
00:06:30.390 --> 00:06:33.723
And as you can see, it's not found, and the reason why

110
00:06:33.723 --> 00:06:37.186
this actually happens is because Scrapy, by default,

111
00:06:37.186 --> 00:06:40.327
so if you go to the root directory, as I was saying,

112
00:06:40.327 --> 00:06:44.494
Scrapy, by default, will go to the start_urls,

113
00:06:44.540 --> 00:06:48.074
and then it will append /robots.txt file to figure out

114
00:06:48.074 --> 00:06:52.241
if there, if well to be a bit more friendlier to the site.

115
00:06:52.859 --> 00:06:54.801
Let's say it like that.

116
00:06:54.801 --> 00:06:58.968
And to figure out the rules of actually what can and cannot

117
00:06:59.373 --> 00:07:00.206
be scraped.

118
00:07:00.206 --> 00:07:04.373
So we want to exclude this so we go to the quotes_spider

119
00:07:05.619 --> 00:07:07.527
and then go to the settings.py.

120
00:07:07.527 --> 00:07:11.694
Open it with your Text Editor, and here, if you see,

121
00:07:12.010 --> 00:07:14.395
there will be comments.

122
00:07:14.395 --> 00:07:18.562
First that, "Obey robots.txt rules", and then 

123
00:07:18.733 --> 00:07:22.408
ROBOSTXT_OBEY = True.

124
00:07:24.749 --> 00:07:27.054
What you need to just do is set it to false,

125
00:07:27.054 --> 00:07:31.221
so type in False and then save it, close it,

126
00:07:31.393 --> 00:07:35.445
and then finally go back to the Terminal, let's see

127
00:07:35.445 --> 00:07:38.441
the Terminal is here and then once again run it.

128
00:07:38.441 --> 00:07:41.938
So scrapy crawl and then the name of the Spider

129
00:07:41.938 --> 00:07:42.893
will be quotes.

130
00:07:42.893 --> 00:07:46.476
Hit Enter and you will see that the message

131
00:07:47.906 --> 00:07:49.313
will be removed.

132
00:07:49.313 --> 00:07:53.133
So previously we have somewhere here message that robots.txt

133
00:07:53.133 --> 00:07:57.300
page was crawled and it returned 404 page status

134
00:07:57.926 --> 00:08:00.947
but currently we don't get that because we excluded

135
00:08:00.947 --> 00:08:02.679
ROBOSTXT_OBEY.

136
00:08:02.679 --> 00:08:06.846
So we just scraped one page and as you can see we scraped

137
00:08:07.355 --> 00:08:11.522
it from here and then we in the dictionary keys

138
00:08:12.006 --> 00:08:16.173
set of values, this is pretty much our yield statement.

139
00:08:17.678 --> 00:08:21.845
And since we scraped it just one page then the INFO message

140
00:08:22.959 --> 00:08:26.229
of closing Spider with the message that it's finished

141
00:08:26.229 --> 00:08:30.396
is displayed right after the scrape and then there will be

142
00:08:30.607 --> 00:08:34.607
some bunch of Scrapy stats so these all messages

143
00:08:35.616 --> 00:08:38.469
are pretty much the stats and here what you

144
00:08:38.469 --> 00:08:42.626
need to really know is that if you are scraping

145
00:08:42.626 --> 00:08:46.694
some large site, for example, you will want to see

146
00:08:46.694 --> 00:08:48.234
the response_status_count.

147
00:08:48.234 --> 00:08:52.286
So since we only scraped one page here, we get 200

148
00:08:52.286 --> 00:08:55.679
as successful, as previously discussed and one as a value,

149
00:08:55.679 --> 00:08:59.846
finish_reason which finished also finish_time and also

150
00:08:59.855 --> 00:09:02.179
you have start_time so you can calculate how much

151
00:09:02.179 --> 00:09:04.488
exactly time it took to run Spider.

152
00:09:04.488 --> 00:09:06.621
item_scraped_count is also important

153
00:09:06.621 --> 00:09:10.788
and we only scraped one data or really item because

154
00:09:11.803 --> 00:09:15.183
this is the page that we scraped.

155
00:09:15.183 --> 00:09:19.350
And let's see DEBUG messages and also the INFO messages

156
00:09:19.381 --> 00:09:22.965
which goes to the log_count, so if you got any kind of,

157
00:09:22.965 --> 00:09:26.317
or if you think there was some error in the Spider code,

158
00:09:26.317 --> 00:09:29.243
you will go to the log_count/ERROR and see

159
00:09:29.243 --> 00:09:32.200
if there were any kind of or how many errors.

160
00:09:32.200 --> 00:09:35.024
There would be but, as you can see in our case,

161
00:09:35.024 --> 00:09:37.796
we didn't get any kind of error.

162
00:09:37.796 --> 00:09:40.368
And that would be pretty much it for this video

163
00:09:40.368 --> 00:09:43.258
and I'll see you in the very next one.

