WEBVTT FILE
Kind: captions
Language: pt

1
00:00:00.589 --> 00:00:04.259
Olá! Neste vídeo nós vamos cobrir este código

2
00:00:04.259 --> 00:00:08.357
do Scrapy que o Scrapy escreve por padrão

3
00:00:08.357 --> 00:00:09.493
para nós.

4
00:00:09.493 --> 00:00:13.567
E ainda nós vamos passar por cima dos dois pontos de dados que extraímos

5
00:00:13.567 --> 00:00:14.440
anteriormente.

6
00:00:14.440 --> 00:00:15.711
Então vamos para este site.

7
00:00:15.711 --> 00:00:18.747
Então isso inclui este ponto de dado e ainda este daqui

8
00:00:18.747 --> 00:00:22.708
e vamos fornecer ele no nosso código Scrapy em algum lugar

9
00:00:22.708 --> 00:00:23.737
na função parse.

10
00:00:23.737 --> 00:00:26.717
Então vamos começar com o próprio código.

11
00:00:26.717 --> 00:00:30.884
Então como você pode ver, o Spider tem a subclasse scrapy.Spider

12
00:00:31.782 --> 00:00:34.868
e define alguns atributos e métodos.

13
00:00:34.868 --> 00:00:39.035
"name", por exemplo, identifica o nome do Spider, obviamente.

14
00:00:39.125 --> 00:00:42.453
Ele deve ser único dentro do projeto, então você não pode

15
00:00:42.453 --> 00:00:46.489
atribuir o mesmo nome para um Spider diferente mesmo que

16
00:00:46.489 --> 00:00:50.377
eles estejam na mesma estrutura do projeto.

17
00:00:50.377 --> 00:00:54.285
"allowed_domains" é uma lista de, obviamente, domínios permitidos.

18
00:00:54.285 --> 00:00:57.895
Se o Scrapy encontra, por exemplo, algum domínio diferente

19
00:00:57.895 --> 00:01:02.062
que não seja quotes.toscrape.com, ele não vai processá-lo

20
00:01:03.209 --> 00:01:05.739
e vai automaticamente filtrá-lo.

21
00:01:05.739 --> 00:01:09.041
Na maior parte do tempo, você vai encontrar isso se você

22
00:01:09.041 --> 00:01:11.611
está rastreando toda URL no site.

23
00:01:11.611 --> 00:01:15.613
"start_urls" é tanto por padrão uma tupla ou você

24
00:01:15.613 --> 00:01:17.738
pode definí-lo como uma lista.

25
00:01:17.738 --> 00:01:21.905
E isso é apenas... isso vai ser a primeira URL

26
00:01:23.022 --> 00:01:25.182
que o Scrapy vai processar.

27
00:01:25.182 --> 00:01:29.349
Você não precisa usar o "www." nessa parte

28
00:01:29.833 --> 00:01:31.083
então vamos excluir isso.

29
00:01:31.083 --> 00:01:34.680
E "parse" é o método padrão callback do Scrapy

30
00:01:34.680 --> 00:01:38.846
no scrapy.Spider, ou seja, no template básico

31
00:01:40.552 --> 00:01:41.547
que o Scrapy oferece.

32
00:01:41.547 --> 00:01:45.714
Então esse método é chamado quando ou para a Request

33
00:01:46.333 --> 00:01:49.117
que não tem um callback atribuído explicitamente.

34
00:01:49.117 --> 00:01:51.554
Definir algum outro nome, por exemplo, não vai funcionar.

35
00:01:51.554 --> 00:01:55.119
Então se você digitar aqui foobar, não vai funcionar ou

36
00:01:55.119 --> 00:01:57.202
o Scrapy vai receber um erro.

37
00:02:00.100 --> 00:02:02.638
Ele tem o "self", obviamente, e o "responde". "self" é

38
00:02:02.638 --> 00:02:06.805
porque ele está em uma classe, certo, e "response" porque

39
00:02:07.080 --> 00:02:11.122
ele vai ser o objeto resposta ou o nó HTML

40
00:02:12.476 --> 00:02:16.059
ou o código-fonte HTML da página.

41
00:02:16.059 --> 00:02:20.226
Vamos salvá-lo como está e vamos voltar para o Terminal.

42
00:02:21.545 --> 00:02:24.102
E aqui nós temos dois pontos de dados.

43
00:02:24.102 --> 00:02:27.335
Então o primeiro deles está em algum lugar aqui em cima
So the first one is somewhere here above so the

44
00:02:27.335 --> 00:02:30.801
então o texto da tag &lt;h1&gt; está localizado aqui.

45
00:02:31.229 --> 00:02:35.396
Então se nós copiarmos essa linha e colar no nosso método parse

46
00:02:38.062 --> 00:02:42.229
e atribuir algo como "h1_tag"

47
00:02:42.743 --> 00:02:46.910
como o nome da variável e isso é igual a esse seletor XPath.

48
00:02:47.003 --> 00:02:51.004
Vamos voltar para o Terminal.

49
00:02:51.004 --> 00:02:54.593
Esse é o segundo ponto de dados, localizado no final,

50
00:02:54.593 --> 00:02:58.759
então se copiarmos e colarmos no nosso código Scrapy

51
00:03:00.290 --> 00:03:04.183
e nomeá-lo, por exemplo, variável "tags" é igual

52
00:03:04.183 --> 00:03:08.350
a esse seletor XPath e vamos usar "yield"

53
00:03:08.470 --> 00:03:10.621
nesses dois items.

54
00:03:10.621 --> 00:03:14.621
Então fornecendo esses dois pontos de dados

55
00:03:15.931 --> 00:03:20.098
nós vamos imprimir a saída do Scrapy e então você

56
00:03:20.369 --> 00:03:24.536
pode pegar esses dados em: CSV, JSON, XML ou qualquer outro

57
00:03:26.758 --> 00:03:27.653
banco de dados.

58
00:03:27.653 --> 00:03:30.962
Então para usar o "yield" nós digitamos, obviamente, a chave do dicionário.

59
00:03:30.962 --> 00:03:35.129
A primeira será a "h1_tag", por exemplo, e, obviamente,

60
00:03:35.459 --> 00:03:38.418
a chave vai ser "H1 Tag".

61
00:03:39.721 --> 00:03:43.509
E então o segundo conjunto de chaves e valores vai ser "tags".

62
00:03:43.509 --> 00:03:47.676
Então digitamos "Tags" como o valor da chave e como o valor,

63
00:03:50.504 --> 00:03:52.153
nós digitamos "tags".

64
00:03:52.153 --> 00:03:56.300
Vamos salvar isso e voltar para o nosso Terminal.

65
00:03:56.300 --> 00:04:00.041
E abrir, na verdade, uma outra aba aqui.

66
00:04:01.145 --> 00:04:02.759
Vamos maximizar isto.

67
00:04:02.759 --> 00:04:06.630
E, primeira coisa, claro, nós precisamos ir

68
00:04:06.630 --> 00:04:08.438
para o diretório raiz.

69
00:04:08.438 --> 00:04:12.214
Então nós mudamos nosso diretório para o Desktop

70
00:04:12.214 --> 00:04:14.231
e então para quotes_spider.

71
00:04:14.231 --> 00:04:18.398
Pressionamos Enter e se o arquivo de configuração estiver aqui, e ele está,

72
00:04:18.841 --> 00:04:22.558
então nós estamos no diretório raiz, o que é ótimo.

73
00:04:22.558 --> 00:04:24.576
Então como você roda ele ou roda

74
00:04:24.576 --> 00:04:28.743
o o código Scrapy do spider é digitando "scrapy".

75
00:04:29.242 --> 00:04:33.409
Você não precisa obviamente fazer isso, mas os comandos estão aqui.

76
00:04:34.525 --> 00:04:37.310
Então "scrapy crawl", e então executa um spider, certo?

77
00:04:37.310 --> 00:04:40.764
Então digite "scrapy crawl" e então

78
00:04:40.764 --> 00:04:42.051
o nome do spider.

79
00:04:42.051 --> 00:04:45.161
O nome do spider pode ser recuperado tanto com

80
00:04:45.161 --> 00:04:48.103
o "scrapy list", que vai imprimir o nome dos dois spider,

81
00:04:48.103 --> 00:04:51.110
então este é o que estamos procurando ou você pode ir

82
00:04:51.110 --> 00:04:55.277
para seu editor de texto e ver que o nome é quotes.

83
00:04:55.371 --> 00:04:58.959
Então vamos copiar isso aqui, voltar para o Terminal.

84
00:04:58.959 --> 00:05:02.206
Digitar "scrapy crawl" para executar,

85
00:05:02.206 --> 00:05:06.373
então o spider Scrapy, e então colar o nome do Spider.

86
00:05:07.576 --> 00:05:09.243
E vamos pressionar Enter.

87
00:05:10.164 --> 00:05:14.331
E aqui nós temos um punhado de saídas e logs

88
00:05:14.545 --> 00:05:18.712
e, como você pode ver, algumas coisas são muito parecidas

89
00:05:19.843 --> 00:05:23.785
com o Scrapy Shell, então o Scrapy foi iniciado, configurações

90
00:05:23.785 --> 00:05:27.035
substituídas, extensões, middlewares, etc.

91
00:05:28.225 --> 00:05:31.646
O que você precisa saber é somente

92
00:05:31.646 --> 00:05:35.386
quando o Scrapy iniciou e quando ele foi finalizado.

93
00:05:35.386 --> 00:05:39.553
Então todos esses dados ou linhas da saída.

94
00:05:40.486 --> 00:05:43.954
Então temos aqui que o spider está aberto, que ele rastreou

95
00:05:43.954 --> 00:05:46.953
no começo, obviamente, zero páginas e você terá

96
00:05:46.953 --> 00:05:51.120
esta mesma mensagem a cada minuto, então você verá

97
00:05:52.277 --> 00:05:56.444
quantas páginas foram rastreadas e também as páginas extraidas.

98
00:05:57.924 --> 00:06:02.091
Telnet console para que você copie e cole este endereço IP

99
00:06:02.098 --> 00:06:03.642
e possa escutá-lo.

100
00:06:03.642 --> 00:06:06.976
E aqui temos a primeira URL que foi extraída

101
00:06:06.976 --> 00:06:10.288
que vai para nossa página inicial e então vai para

102
00:06:10.288 --> 00:06:12.600
o /robots.txt.

103
00:06:12.600 --> 00:06:16.767
Esta página não está presente e esse é o motivo que

104
00:06:17.144 --> 00:06:18.656
recebemos 404.

105
00:06:18.656 --> 00:06:22.664
E então o segundo é bem sucedido e só vai

106
00:06:22.664 --> 00:06:23.664
para a página inicial.

107
00:06:23.664 --> 00:06:27.170
Então vamos copiar este link e então navegar para o

108
00:06:27.170 --> 00:06:30.390
Google Chrome e então colar isto aqui.

109
00:06:30.390 --> 00:06:33.723
E como você pode ver, ele não foi encontrado, e o motivo

110
00:06:33.723 --> 00:06:37.186
disso acontecer é porque o Scrapy, por padrão,

111
00:06:37.186 --> 00:06:40.327
então se você for para o diretório raiz, como eu estava dizendo,

112
00:06:40.327 --> 00:06:44.494
Scrapy, por padrão, vai para o start_urls,

113
00:06:44.540 --> 00:06:48.074
e então vai anexar o arquivo /robots.txt para descobrir

114
00:06:48.074 --> 00:06:52.241
se... bem, para ser mais amigável ao site.

115
00:06:52.859 --> 00:06:54.801
Vamos dizer isso.

116
00:06:54.801 --> 00:06:58.968
E para descobrir as regras do que pode e o que não pode

117
00:06:59.373 --> 00:07:00.206
ser extraído.

118
00:07:00.206 --> 00:07:04.373
Então nós queremos excluir isso então vamos para o quotes_spider

119
00:07:05.619 --> 00:07:07.527
e então para o settings.py.

120
00:07:07.527 --> 00:07:11.694
Abra no seu editor de texto, e aqui, como você vê,

121
00:07:12.010 --> 00:07:14.395
vai ter comments.

122
00:07:14.395 --> 00:07:18.562
Primeiro, "Obey robots.txt rules", e então

123
00:07:18.733 --> 00:07:22.408
ROBOSTXT_OBEY = True.

124
00:07:24.749 --> 00:07:27.054
O que você precisa fazer é colocar isso como falso,

125
00:07:27.054 --> 00:07:31.221
então digite "False" e então salve, feche,

126
00:07:31.393 --> 00:07:35.445
e então vamos finalmente voltar para o terminal, vamos ver.

127
00:07:35.445 --> 00:07:38.441
O Terminal está  aqui e mais uma vez rodá-lo.

128
00:07:38.441 --> 00:07:41.938
Então "scrapy crawl" e o nome do spider

129
00:07:41.938 --> 00:07:42.893
vai ser quotes.

130
00:07:42.893 --> 00:07:46.476
Pressione Enter e você verá que a mensagem

131
00:07:47.906 --> 00:07:49.313
será removida.

132
00:07:49.313 --> 00:07:53.133
Então anteriormente nós tínhamos em algum lugar que a página

133
00:07:53.133 --> 00:07:57.300
robots.txt foi extraída e ela retournou o status 404

134
00:07:57.926 --> 00:08:00.947
mas atualmente não temos ela porque nós excluímos

135
00:08:00.947 --> 00:08:02.679
ROBOSTXT_OBEY.

136
00:08:02.679 --> 00:08:06.846
Então só extraímos uma página e, como você pode ver, extraímos

137
00:08:07.355 --> 00:08:11.522
daqui e então no conjunto de valores de chaves

138
00:08:12.006 --> 00:08:16.173
do dicionário, essa é nossa declaração yield.

139
00:08:17.678 --> 00:08:21.845
E como nós extraímos somente uma página então a mensagem INFO

140
00:08:22.959 --> 00:08:26.229
do fim do Spider com a mensagem de ele foi finalizado

141
00:08:26.229 --> 00:08:30.396
é mostrado logo depois do scrape e então haverá

142
00:08:30.607 --> 00:08:34.607
algumas estatísticas do Scrapy então todas estas mensagens

143
00:08:35.616 --> 00:08:38.469
são as estatísticas e aqui o que você precisa

144
00:08:38.469 --> 00:08:42.626
realmente saber é que se você está extraindo

145
00:08:42.626 --> 00:08:46.694
algum site grande, por exemplo, você vai querer ver

146
00:08:46.694 --> 00:08:48.234
o response_status_count.

147
00:08:48.234 --> 00:08:52.286
Então já que extraímos somente uma página aqui, nós temos 200

148
00:08:52.286 --> 00:08:55.679
como bem sucedido, como previamente discutido, e 1 como valor,

149
00:08:55.679 --> 00:08:59.846
finish_reason com finished e finish_time e também

150
00:08:59.855 --> 00:09:02.179
você tem start_time para que você possa calcular quanto

151
00:09:02.179 --> 00:09:04.488
tempo exatamente ele levou para rodar o Spider.

152
00:09:04.488 --> 00:09:06.621
O item_scraped.count é importante também

153
00:09:06.621 --> 00:09:10.788
e nós nós extraímos um dado ou um item porque

154
00:09:11.803 --> 00:09:15.183
esta é a página que extraímos.

155
00:09:15.183 --> 00:09:19.350
E vamos ver as mensagens DEBUG e também as mensagens INFO

156
00:09:19.381 --> 00:09:22.965
que vai para o log_count, então se você receber algum,

157
00:09:22.965 --> 00:09:26.317
ou se você acha que tem algum erro no código do Spider,

158
00:09:26.317 --> 00:09:29.243
você vai para o log_counter/ERROR e vê

159
00:09:29.243 --> 00:09:32.200
se tem algum tipo ou quantos erros tem.

160
00:09:32.200 --> 00:09:35.024
Poderia ter, mas, como você pode ver no nosso caso,

161
00:09:35.024 --> 00:09:37.796
não tivemos nenhum tipo de erro.

162
00:09:37.796 --> 00:09:40.368
Então isso é tudo para este vídeo

163
00:09:40.368 --> 00:09:43.258
e vejo você no próximo.

