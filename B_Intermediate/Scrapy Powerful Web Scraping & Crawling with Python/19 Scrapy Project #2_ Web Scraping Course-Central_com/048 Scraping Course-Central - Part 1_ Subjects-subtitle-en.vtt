WEBVTT

1
00:00:00.770 --> 00:00:01.603
Hey there!

2
00:00:01.603 --> 00:00:03.956
So today, we are going to build a spider.

3
00:00:03.956 --> 00:00:08.123
That spider will go to the class-central.com/subjects

4
00:00:08.246 --> 00:00:11.318
and it will either scrape entire courses,

5
00:00:11.318 --> 00:00:14.495
so in this case there are 12 courses,

6
00:00:14.495 --> 00:00:18.662
so spider will go into each and every bolded URL,

7
00:00:18.913 --> 00:00:22.356
as you can see, currently there are again 12 of them,

8
00:00:22.356 --> 00:00:24.697
or if we provide an argument,

9
00:00:24.697 --> 00:00:27.116
which can be, for example, a subject,

10
00:00:27.116 --> 00:00:28.932
and set it to "Programming",

11
00:00:28.932 --> 00:00:31.777
in this case, the spider will go to the corresponding URL,

12
00:00:31.777 --> 00:00:33.690
which is going to be this URL,

13
00:00:33.690 --> 00:00:37.857
and it will scrape, in this case, 488 courses.

14
00:00:38.327 --> 00:00:39.178
So let's begin.

15
00:00:39.178 --> 00:00:40.881
So of course, the first that we need to do

16
00:00:40.881 --> 00:00:45.048
is go to the Desktop, or really, your work environment,

17
00:00:45.965 --> 00:00:48.051
open Terminal, in this case

18
00:00:48.051 --> 00:00:51.554
so since we are going to as always work on our Desktop

19
00:00:51.554 --> 00:00:54.422
we will change directory to the Desktop

20
00:00:54.422 --> 00:00:57.815
and we will then generate our project,

21
00:00:57.815 --> 00:01:00.130
so scrapy startproject.

22
00:01:00.130 --> 00:01:03.380
Let's actually zoom it in a little bit.

23
00:01:05.397 --> 00:01:08.397
So scrapy startproject and finally

24
00:01:09.753 --> 00:01:12.815
the name of the project can be class_central

25
00:01:12.815 --> 00:01:16.258
so Class Central is the name of site

26
00:01:16.258 --> 00:01:18.597
and append just '_spider' for example,

27
00:01:18.597 --> 00:01:21.500
and we will change the directory

28
00:01:21.500 --> 00:01:24.648
to the class_central_spider folder

29
00:01:24.648 --> 00:01:28.621
and then we can pretty much generate our spider of course

30
00:01:28.621 --> 00:01:32.419
so scrapy genspider the name of the spider can be,

31
00:01:32.419 --> 00:01:36.586
for example, subjects and the site, of course,

32
00:01:36.828 --> 00:01:38.828
is the class-central.com

33
00:01:40.501 --> 00:01:44.160
Of course, this is going to be used in the allowed_domains

34
00:01:44.160 --> 00:01:47.070
variable name, as you will see in a moment,

35
00:01:47.070 --> 00:01:50.153
so if we go back to Desktop and go in

36
00:01:50.988 --> 00:01:54.180
and go to the root directory, so if we go to the folder

37
00:01:54.180 --> 00:01:57.949
let's set it - or let's go to the settings.py

38
00:01:57.949 --> 00:02:00.866
and set the ROBOTSTXT_OBEY to False

39
00:02:01.984 --> 00:02:05.355
so we don't get any issues or restrictions

40
00:02:05.355 --> 00:02:09.522
and go to the spiders folder and subject.py

41
00:02:09.661 --> 00:02:12.576
is the name of our spider

42
00:02:12.576 --> 00:02:15.876
and so if we prettify it a little bit

43
00:02:15.876 --> 00:02:18.293
so from scrapy import Spider,

44
00:02:19.642 --> 00:02:22.394
this is, just by the way, my preference,

45
00:02:22.394 --> 00:02:25.061
since I like to have everything,

46
00:02:27.112 --> 00:02:29.101
or in this case, absolute imports

47
00:02:29.101 --> 00:02:32.768
and also I tend to use always single quotes.

48
00:02:33.846 --> 00:02:37.574
You can use, of course, single or double quotes,

49
00:02:37.574 --> 00:02:41.239
again, my preference is just to use single quotes.

50
00:02:41.239 --> 00:02:45.322
start_urls is going to be set not to the homepage

51
00:02:47.539 --> 00:02:49.177
but to the subjects,

52
00:02:49.177 --> 00:02:51.445
so we will append subjects.

53
00:02:51.445 --> 00:02:54.048
So if we go back, and let's see,

54
00:02:54.048 --> 00:02:56.289
so I think I haven't saved it,

55
00:02:56.289 --> 00:02:59.158
so save it and finally let's run it,

56
00:02:59.158 --> 00:03:01.692
just to verify that everything will work correctly,

57
00:03:01.692 --> 00:03:04.632
so scrapy crawl and then the name of the spider,

58
00:03:04.632 --> 00:03:07.069
which is going to be subjects,

59
00:03:07.069 --> 00:03:09.152
so scrapy crawl subjects.

60
00:03:10.275 --> 00:03:11.814
Let's see if this is working.

61
00:03:11.814 --> 00:03:12.689
Perfect.

62
00:03:12.689 --> 00:03:13.928
It works.

63
00:03:13.928 --> 00:03:18.095
So once again to reiterate what exactly we are going to do,

64
00:03:18.269 --> 00:03:21.868
is if argument such as subject is going to be provided

65
00:03:21.868 --> 00:03:25.745
and set for example to social sciences,

66
00:03:25.745 --> 00:03:29.828
then we will fetch this URL, so if we go into it,

67
00:03:31.162 --> 00:03:33.360
you will see that this is the URL, pretty much,

68
00:03:33.360 --> 00:03:35.474
that we will fetch, and then scrape

69
00:03:35.474 --> 00:03:38.640
and then iterate over pages of results

70
00:03:38.640 --> 00:03:41.603
so that if that is the case,

71
00:03:41.603 --> 00:03:45.770
then in that case the site is a bit slower, by the way,

72
00:03:46.012 --> 00:03:50.179
then in that case we will scrape 480 courses, for example,

73
00:03:51.360 --> 00:03:55.030
if we type in Engineering as a subject.

74
00:03:55.030 --> 00:03:57.171
If not, then we will pretty much get

75
00:03:57.171 --> 00:03:59.215
all of the different courses.

76
00:03:59.215 --> 00:04:01.538
As of right now, there are twelve of them.

77
00:04:01.538 --> 00:04:05.705
So the logic, first, is that we will left the parse function

78
00:04:08.818 --> 00:04:11.116
as it is, because we will actually use it.

79
00:04:11.116 --> 00:04:15.204
So for providing arguments, we will will initialize first,

80
00:04:15.204 --> 00:04:18.167
so we will write our __init__ function.

81
00:04:18.167 --> 00:04:20.173
This is already covered.

82
00:04:20.173 --> 00:04:23.837
So self and then the second argument can be subject,

83
00:04:23.837 --> 00:04:27.504
which is going to be set to None by default,

84
00:04:29.546 --> 00:04:32.285
and we can just go, the first thing to note

85
00:04:32.285 --> 00:04:35.658
is just set the indentations to spaces

86
00:04:35.658 --> 00:04:39.322
if there is mixture of the tabs and spaces.

87
00:04:39.322 --> 00:04:43.489
In Sublime Text sometimes there is a mixture of it,

88
00:04:44.170 --> 00:04:48.337
so to keep everything clean, just indent to spaces only.

89
00:04:51.306 --> 00:04:54.955
So going back to it, we can type in self.subject

90
00:04:54.955 --> 00:04:58.425
is equal to the subject, and that will be pretty much it.

91
00:04:58.425 --> 00:05:01.950
So we initialized our subject variable.

92
00:05:01.950 --> 00:05:05.129
The way that we will set the logic

93
00:05:05.129 --> 00:05:09.296
here in the parse function is if self.subject is present

94
00:05:10.905 --> 00:05:14.416
we will print out, for now, True.

95
00:05:14.416 --> 00:05:16.916
Else, we will print out False.

96
00:05:19.017 --> 00:05:22.635
The reason why we will do this is to test out

97
00:05:22.635 --> 00:05:25.704
if our logic is working correctly and then save it,

98
00:05:25.704 --> 00:05:29.408
go back to the Terminal, and then run it once again.

99
00:05:29.408 --> 00:05:32.708
So this should print out the False message,

100
00:05:32.708 --> 00:05:35.632
as you will see in a moment, and here it is.

101
00:05:35.632 --> 00:05:37.840
The reason why, of course, is because

102
00:05:37.840 --> 00:05:40.598
we haven't specified the subject.

103
00:05:40.598 --> 00:05:44.765
So if we specify the subject by typing in -a subject

104
00:05:46.320 --> 00:05:50.487
is equal to, whatever, foo, the message will be True,

105
00:05:50.618 --> 00:05:52.295
as you will see in a moment.

106
00:05:52.295 --> 00:05:53.689
So here is the True message,

107
00:05:53.689 --> 00:05:57.856
so our logic is working correctly, which is perfect.

108
00:05:57.961 --> 00:06:01.878
The next thing that we will do is fetch the URL

109
00:06:03.079 --> 00:06:06.996
if the subject argument is going to be present.

110
00:06:07.976 --> 00:06:12.139
So to do that, let's just use for example, Programming.

111
00:06:12.139 --> 00:06:14.775
Go to the inspect, and you will see here

112
00:06:14.775 --> 00:06:18.903
that we have the &lt;a&gt; tag, and here is our href,

113
00:06:18.903 --> 00:06:21.975
or really the value that we are after.

114
00:06:21.975 --> 00:06:26.142
And if we - there are multiple ways of selecting this URL,

115
00:06:26.444 --> 00:06:30.072
really, the way that we will approach this

116
00:06:30.072 --> 00:06:32.007
is just to use title.

117
00:06:32.007 --> 00:06:35.479
So if title contains the Programming in it,

118
00:06:35.479 --> 00:06:39.415
we will fetch the href from the &lt;a&gt; tag itself.

119
00:06:39.415 --> 00:06:40.791
Pretty simple.

120
00:06:40.791 --> 00:06:44.247
So to do that, of course, we will use Scrapy Shell.

121
00:06:44.247 --> 00:06:48.382
So if we copy and go back to the Terminal

122
00:06:48.382 --> 00:06:51.349
and open additionally new Tab

123
00:06:53.383 --> 00:06:55.082
and change the directory to the home one

124
00:06:55.082 --> 00:06:57.806
and then type in scrapy shell

125
00:06:57.806 --> 00:07:01.532
and let's also maximize this window,

126
00:07:01.532 --> 00:07:04.332
so scrapy shell, or zoom it in,

127
00:07:04.332 --> 00:07:07.723
and in either single or double quotes,

128
00:07:07.723 --> 00:07:10.572
just provide the URL and hit Enter

129
00:07:10.572 --> 00:07:11.964
and go back to the site.

130
00:07:11.964 --> 00:07:15.356
So once again, so if there is a title

131
00:07:15.356 --> 00:07:19.165
which contains Programming, for example,

132
00:07:19.165 --> 00:07:22.396
then we will fetch the href itself,

133
00:07:22.396 --> 00:07:23.932
so really simple.

134
00:07:23.932 --> 00:07:26.508
So the way that you actually fetch this

135
00:07:26.508 --> 00:07:30.675
or get to the programming href is going to be response.xpath

136
00:07:32.321 --> 00:07:36.488
and then in parentheses we will type in the single quotes

137
00:07:37.409 --> 00:07:40.762
and then, so double slashes, asterisk,

138
00:07:40.762 --> 00:07:44.905
and then in the square brackets we will use contains

139
00:07:44.905 --> 00:07:48.555
and then in, again, parentheses, we will use title.

140
00:07:48.555 --> 00:07:51.888
So this is the title value we are after,

141
00:07:52.953 --> 00:07:56.345
so if it contains the Programming, we will, again,

142
00:07:56.345 --> 00:07:58.601
fetch the href itself.

143
00:07:58.601 --> 00:08:02.768
So title, and then comma and so we will type in Programming.

144
00:08:06.848 --> 00:08:11.015
And let's hit Enter and let's see if this is working.

145
00:08:11.399 --> 00:08:14.054
So here, this is selecting the &lt;a&gt; class,

146
00:08:14.054 --> 00:08:18.119
as you can see in double quotes there is

147
00:08:18.119 --> 00:08:21.910
show-all-subjects view-all- probably columns.

148
00:08:21.910 --> 00:08:25.590
So if we go back to the site you will see

149
00:08:25.590 --> 00:08:28.326
that this is pretty much the corresponding value,

150
00:08:28.326 --> 00:08:31.272
or HTML node that we are currently selected,

151
00:08:31.272 --> 00:08:33.738
so the only thing that we need to do

152
00:08:33.738 --> 00:08:35.094
is fetch the href itself

153
00:08:35.094 --> 00:08:37.622
because we don't need the class name.

154
00:08:37.622 --> 00:08:40.892
So to do that, of course, we go back to our Shell

155
00:08:40.892 --> 00:08:45.059
and then just use href and as you will see in a moment

156
00:08:46.884 --> 00:08:49.540
so this is the data that we are after,

157
00:08:49.540 --> 00:08:53.707
so of course we type in finally that 

158
00:08:54.677 --> 00:08:57.077
extract_first()

159
00:08:57.077 --> 00:09:01.244
and this is going - or it's not going to be our final URL,

160
00:09:01.489 --> 00:09:05.656
it's going to be the absolute URL will have to be added

161
00:09:05.982 --> 00:09:10.149
or the homepage or the, really, this part of the URL

162
00:09:11.876 --> 00:09:15.636
will have to be added, which we will do in a moment or so

163
00:09:15.636 --> 00:09:18.612
let's copy and paste this as of right now

164
00:09:18.612 --> 00:09:22.779
and go back to our code and set this variable name to,

165
00:09:23.204 --> 00:09:25.287
for example, subject_url.

166
00:09:28.059 --> 00:09:31.892
So subject_url is going to be equal to the response.xpath

167
00:09:31.892 --> 00:09:35.620
and we are not going to use always Programming,

168
00:09:35.620 --> 00:09:39.188
because the subject will be dynamically added,

169
00:09:39.188 --> 00:09:42.605
and so to pretty much get to the subject,

170
00:09:44.568 --> 00:09:47.485
we will enter here the self.subject

171
00:09:49.124 --> 00:09:51.700
so this will be, depending on our subject,

172
00:09:51.700 --> 00:09:54.804
this will be additionally added,

173
00:09:54.804 --> 00:09:58.101
so if we change the subject, for example, for Art &amp; Design,

174
00:09:58.101 --> 00:10:01.434
we will get the Art &amp; Design URL itself.

175
00:10:02.546 --> 00:10:05.060
Let's go back to our code

176
00:10:05.060 --> 00:10:08.467
and finally we need to yield our Request

177
00:10:08.467 --> 00:10:12.244
but before that we can just import Request itself

178
00:10:12.244 --> 00:10:16.002
so from scrapy.http import Request

179
00:10:19.331 --> 00:10:23.498
and so we yield Request, the URL itself has to be joined,

180
00:10:28.451 --> 00:10:32.609
so we will use our response.urljoin,

181
00:10:33.231 --> 00:10:37.398
so let's actually test it out first on the Scrapy Shell,

182
00:10:38.045 --> 00:10:42.212
so response.urljoin, and then, pretty much, the URL

183
00:10:45.521 --> 00:10:48.353
or part of the URL is going to be this one,

184
00:10:48.353 --> 00:10:50.881
so if we hit Enter, you will see that

185
00:10:50.881 --> 00:10:53.811
this is the URL that is going to be added,

186
00:10:53.811 --> 00:10:56.753
so if we copy, and let's see,

187
00:10:56.753 --> 00:10:59.586
paste - so if we paste it here,

188
00:11:02.959 --> 00:11:05.709
and have the, so this is the URL,

189
00:11:07.206 --> 00:11:09.605
so copy, paste it right now,

190
00:11:09.605 --> 00:11:11.828
we will see that this is the URL

191
00:11:11.828 --> 00:11:13.815
corresponding to the Programming

192
00:11:13.815 --> 00:11:15.611
and the site is a bit slower,

193
00:11:15.611 --> 00:11:17.892
so don't worry about that,

194
00:11:17.892 --> 00:11:21.013
so this is working perfectly fine.

195
00:11:21.013 --> 00:11:24.869
Let's see, so the last thing that we need to do

196
00:11:24.869 --> 00:11:29.036
is, again, go back to the code - so response.urljoin

197
00:11:30.629 --> 00:11:34.796
and then set the subject_url to be the really absolute URL

198
00:11:38.168 --> 00:11:42.116
and the callback function can be, for example,

199
00:11:42.116 --> 00:11:46.283
self.parse_subject, so we define the subject function.

200
00:11:50.901 --> 00:11:55.068
So def parse_subject(self, response) and for now

201
00:11:55.732 --> 00:11:59.815
we will pass it and save it finally if we go back

202
00:12:01.108 --> 00:12:05.275
and go to the root directory and finally type in

203
00:12:05.941 --> 00:12:09.364
something like Programming in the subject,

204
00:12:09.364 --> 00:12:12.324
we will see that, let's see,

205
00:12:12.324 --> 00:12:13.492
if there is something missing here,

206
00:12:13.492 --> 00:12:14.742
which I missed,

207
00:12:16.068 --> 00:12:17.828
that should be pretty much it.

208
00:12:17.828 --> 00:12:20.308
Once again, scrapy crawl subjects -a

209
00:12:20.308 --> 00:12:22.728
subject is going to be set to Programming.

210
00:12:22.728 --> 00:12:26.645
This, in our case, will go to pretty much this URL,

211
00:12:26.645 --> 00:12:29.412
and it will just pass it as it is.

212
00:12:29.412 --> 00:12:32.495
So hit Enter and here we see the URL,

213
00:12:35.092 --> 00:12:38.683
it successfully fetched, so everything is working correctly.

214
00:12:38.683 --> 00:12:40.676
And for the end of this video,

215
00:12:40.676 --> 00:12:42.997
we will just cover the else logic.

216
00:12:42.997 --> 00:12:45.910
So if subject_url is not provided, in this case

217
00:12:45.910 --> 00:12:50.077
we will fetch, as said previously, twelve different courses.

218
00:12:55.418 --> 00:12:58.465
To do that, go to the Inspect

219
00:12:58.465 --> 00:13:02.632
and really fetching this URL couldn't be simpler.

220
00:13:02.682 --> 00:13:06.849
We will see that in the class, or &lt;a&gt; tag and in the class

221
00:13:07.821 --> 00:13:11.946
you will see show-all-subjects view-all-courses

222
00:13:11.946 --> 00:13:13.563
is going to be class name.

223
00:13:13.563 --> 00:13:17.725
So if we copy, the way really, to quickly test out

224
00:13:17.725 --> 00:13:20.826
if you will fetch all of them,

225
00:13:20.826 --> 00:13:23.354
is to quickly copy and then paste it really,

226
00:13:23.354 --> 00:13:24.666
and see that here we found twelve instances

227
00:13:24.666 --> 00:13:26.426
which corresponds to the number of courses,

228
00:13:26.426 --> 00:13:27.722
which is perfect.

229
00:13:27.722 --> 00:13:31.889
So let's go to the Shell, just to test it out and verify,

230
00:13:32.330 --> 00:13:35.413
so response.xpath so in single quotes

231
00:13:37.455 --> 00:13:41.386
find every instance of the class which has,

232
00:13:41.386 --> 00:13:44.526
so if we copy and paste this value,

233
00:13:44.526 --> 00:13:46.693
so copy and then paste it,

234
00:13:47.693 --> 00:13:50.331
you will see that 12 instances,

235
00:13:50.331 --> 00:13:52.916
or 12 selectors in this case,

236
00:13:52.916 --> 00:13:56.730
are going to be selected, and we will just get the href

237
00:13:56.730 --> 00:14:00.313
and from the href, we will call in extract().

238
00:14:02.388 --> 00:14:05.555
So if we copy and paste this statement

239
00:14:06.753 --> 00:14:10.586
and set variable name in the else logic to be,

240
00:14:11.585 --> 00:14:15.752
let's see, subjects, and also let's provide the argument,

241
00:14:18.897 --> 00:14:21.473
or, I'm sorry, the log message.

242
00:14:21.473 --> 00:14:25.640
To do that, we type in self.logger.info and we will just

243
00:14:27.695 --> 00:14:31.553
print out 'Scraping all subjects.'

244
00:14:33.808 --> 00:14:36.225
And so there are 12 subjects,

245
00:14:37.744 --> 00:14:41.360
now these subjects are of course, partial URL.

246
00:14:41.360 --> 00:14:44.624
So to get to the absolute URL, we will of course use

247
00:14:44.624 --> 00:14:48.791
response.urljoin, so we will write simple for loop.

248
00:14:49.424 --> 00:14:53.341
So, for subject in subjects, then yield Request

249
00:15:00.750 --> 00:15:04.917
like we did here, with the subject URL if it's provided.

250
00:15:05.214 --> 00:15:09.310
So yield Request and then response.urljoin

251
00:15:09.310 --> 00:15:12.318
and then, the, in the parentheses, of course,

252
00:15:12.318 --> 00:15:14.254
the subject will be added,

253
00:15:14.254 --> 00:15:17.087
and callback will also be the same as here,

254
00:15:17.087 --> 00:15:21.254
so callback is going to be, let's copy and paste this,

255
00:15:21.342 --> 00:15:23.646
so callback is going to be parse_subject,

256
00:15:23.646 --> 00:15:26.126
so that should be pretty much it.

257
00:15:26.126 --> 00:15:28.527
Save it, go back to the Terminal.

258
00:15:28.527 --> 00:15:30.862
Go back to the root directory,

259
00:15:30.862 --> 00:15:35.029
and then, for now let's not provide the argument,

260
00:15:35.646 --> 00:15:37.694
so this should print out

261
00:15:37.694 --> 00:15:40.830
that we are scraping all of the subjects,

262
00:15:40.830 --> 00:15:42.878
so here, if you see,

263
00:15:42.878 --> 00:15:47.027
this is the logger message that we previously defined.

264
00:15:47.777 --> 00:15:51.919
And if we go back to the Shell itself,

265
00:15:52.734 --> 00:15:56.484
we will see that the messages or the crawling

266
00:15:57.470 --> 00:16:01.637
of the different, all of the different courses or subjects

267
00:16:01.886 --> 00:16:04.590
is started and working correctly.

268
00:16:04.590 --> 00:16:07.057
So that will be pretty much it for this video.

269
00:16:07.057 --> 00:16:09.647
In the very next one we will finish this spider off

270
00:16:09.647 --> 00:16:12.749
by scraping the course name, subject name,

271
00:16:12.749 --> 00:16:16.841
and also absolute course URL and we will iterate

272
00:16:16.841 --> 00:16:20.957
over pages of results if there are more than 50 courses.

273
00:16:21.057 --> 00:16:21.890
See you then!

