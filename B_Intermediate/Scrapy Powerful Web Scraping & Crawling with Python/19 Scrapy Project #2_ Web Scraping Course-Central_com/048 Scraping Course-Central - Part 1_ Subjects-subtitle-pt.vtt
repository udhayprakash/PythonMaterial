WEBVTT FILE
Kind: captions
Language: pt

1
00:00:00.770 --> 00:00:01.603
Olá!

2
00:00:01.603 --> 00:00:03.956
Então hoje nós vamos construir um spider.

3
00:00:03.956 --> 00:00:08.123
Esse spider vai para o class-central.com/subjects

4
00:00:08.246 --> 00:00:11.318
e ele vai extrair o curso inteiro,

5
00:00:11.318 --> 00:00:14.495
então neste caso há 12 cursos,,

6
00:00:14.495 --> 00:00:18.662
então o spider vai para cada URL em negrito,

7
00:00:18.913 --> 00:00:22.356
como você pode ver, há mais uma vez 12 deles atualmente,

8
00:00:22.356 --> 00:00:24.697
ou se fornecermos um argumento,

9
00:00:24.697 --> 00:00:27.116
que vai ser, por exemplo, um assunto,

10
00:00:27.116 --> 00:00:28.932
e definir ele como "Programming".

11
00:00:28.932 --> 00:00:31.777
Neste caso, o spider vai para a URL correspondente,

12
00:00:31.777 --> 00:00:33.690
que vai ser esta URL,

13
00:00:33.690 --> 00:00:37.857
e ele vai extrair, neste caso, 488 cursos.

14
00:00:38.327 --> 00:00:39.178
Então vamos começar.

15
00:00:39.178 --> 00:00:40.881
Então, claro, a primeira coisa que precisamos fazer

16
00:00:40.881 --> 00:00:45.048
é ir para o Desktop ou, na verdade, nosso ambiente de trabalho,

17
00:00:45.965 --> 00:00:48.051
abrir o Terminal, neste caso

18
00:00:48.051 --> 00:00:51.554
como estamos indo sempre trabalhar no nosso Desktop,

19
00:00:51.554 --> 00:00:54.422
vamos mudar o diretório para o Desktop

20
00:00:54.422 --> 00:00:57.815
e vamos então gerar nosso projeto,

21
00:00:57.815 --> 00:01:00.130
então "scrapy startproject".

22
00:01:00.130 --> 00:01:03.380
Então vamos dar um pouco de zoom.

23
00:01:05.397 --> 00:01:08.397
Então "scrapy startproject" e finalmente

24
00:01:09.753 --> 00:01:12.815
o nome do projeto pode ser class_central.

25
00:01:12.815 --> 00:01:16.258
Então Class Central é o nome do site

26
00:01:16.258 --> 00:01:18.597
e anexe apenas "_spider" por exemplo,

27
00:01:18.597 --> 00:01:21.500
e vamos mudar o diretório

28
00:01:21.500 --> 00:01:24.648
para a pasta class_central_spider

29
00:01:24.648 --> 00:01:28.621
e então podemos basicamente gerar nosso spider, claro,

30
00:01:28.621 --> 00:01:32.419
então "scrapy genspider", o nome do spider pode ser,

31
00:01:32.419 --> 00:01:36.586
por exemplo, subjects e o site, claro,

32
00:01:36.828 --> 00:01:38.828
é o class-central.com.

33
00:01:40.501 --> 00:01:44.160
Claro, isto vai ser usado no nome de váriavel allowed_domains,

34
00:01:44.160 --> 00:01:47.070
como você verá em um momento.

35
00:01:47.070 --> 00:01:50.153
Então se voltarmos para o Desktop e ir 

36
00:01:50.988 --> 00:01:54.180
para o diretório raiz... então se nós formos para a pasta,

37
00:01:54.180 --> 00:01:57.949
vamos ver... vamos para o settings.py

38
00:01:57.949 --> 00:02:00.866
e definir o ROBOTS_OBEY como False

39
00:02:01.984 --> 00:02:05.355
para que não tenhamos nenhum problema ou restrição

40
00:02:05.355 --> 00:02:09.522
e vamos para a pasta Spider e subjects.py

41
00:02:09.661 --> 00:02:12.576
é o nome do nosso spider

42
00:02:12.576 --> 00:02:15.876
e então se deixar um pouco mais bonito.

43
00:02:15.876 --> 00:02:18.293
Então "from scrapy import Spider",

44
00:02:19.642 --> 00:02:22.394
isto é, a propósito, minha preferência,

45
00:02:22.394 --> 00:02:25.061
como eu gosto de ter tudo,

46
00:02:27.112 --> 00:02:29.101
ou neste caso, imports absolutos

47
00:02:29.101 --> 00:02:32.768
e também gosto de usar sempre aspas simples.

48
00:02:33.846 --> 00:02:37.574
Você pode usar, claro, aspas simples ou duplas,

49
00:02:37.574 --> 00:02:41.239
novamente, minha preferência é apenas usar aspas simples.

50
00:02:41.239 --> 00:02:45.322
start_urls vai ser definido não para a página inicial,

51
00:02:47.539 --> 00:02:49.177
mas para o subjects,

52
00:02:49.177 --> 00:02:51.445
então vamos adicionar subjects.

53
00:02:51.445 --> 00:02:54.048
Então se voltarmos e, vamos ver,

54
00:02:54.048 --> 00:02:56.289
então eu acho que eu não salvei ele.

55
00:02:56.289 --> 00:02:59.158
Então salve ele e finalmente vamos rodá-lo,

56
00:02:59.158 --> 00:03:01.692
apenas para verificar que tudo vai funcionar corretamente,

57
00:03:01.692 --> 00:03:04.632
então "scrapy crawl" e então o nome do spider,

58
00:03:04.632 --> 00:03:07.069
que vai ser subjects,

59
00:03:07.069 --> 00:03:09.152
então "scrapy crawl subjects".

60
00:03:10.275 --> 00:03:11.814
Vamos ver se isso está funcionando.

61
00:03:11.814 --> 00:03:12.689
Perfeito.

62
00:03:12.689 --> 00:03:13.928
Isso funciona.

63
00:03:13.928 --> 00:03:18.095
Então novamente, para reiterar o que exatamente vamos fazer,

64
00:03:18.269 --> 00:03:21.868
é se o argumento como subject for fornecido

65
00:03:21.868 --> 00:03:25.745
e definido, por exemplo, para Social Sciences,

66
00:03:25.745 --> 00:03:29.828
então ele vai procurar essa URL, então se formos nele,

67
00:03:31.162 --> 00:03:33.360
você verá que esta é basicamente a URL

68
00:03:33.360 --> 00:03:35.474
que vamos buscar e então extrair

69
00:03:35.474 --> 00:03:38.640
e então iterar sobre as páginas dos resultados

70
00:03:38.640 --> 00:03:41.603
então se esse for o caso,

71
00:03:41.603 --> 00:03:45.770
então nesse caso o site é um pouco mais lento, a propósito,

72
00:03:46.012 --> 00:03:50.179
então nesse caso vamos extrair 480 cursos, por exemplo,

73
00:03:51.360 --> 00:03:55.030
se digitarmos Engeneering como um assunto.

74
00:03:55.030 --> 00:03:57.171
Senão, nós vamos basicamente pegar

75
00:03:57.171 --> 00:03:59.215
todos os cursos diferentes.

76
00:03:59.215 --> 00:04:01.538
Neste momento, existem doze deles.

77
00:04:01.538 --> 00:04:05.705
Então a lógica, primeiramente, é que deixemos a função parse

78
00:04:08.818 --> 00:04:11.116
como está, porque vamos, na verdade, usá-la.

79
00:04:11.116 --> 00:04:15.204
Então para fornecer argumentos, vamos inicializar primeiro,

80
00:04:15.204 --> 00:04:18.167
então vamos escrever nossa função __init__,

81
00:04:18.167 --> 00:04:20.173
que já foi coberto.

82
00:04:20.173 --> 00:04:23.837
Então self e então o segundo argumente pode ser subject,

83
00:04:23.837 --> 00:04:27.504
que vai ser definido como None por padrão,

84
00:04:29.546 --> 00:04:32.285
e então pode ir. A primeira coisa para notar

85
00:04:32.285 --> 00:04:35.658
é para definir as identações como espaços

86
00:04:35.658 --> 00:04:39.322
se há uma mistura de tabs e espaços.

87
00:04:39.322 --> 00:04:43.489
No Sublime Text, às vezes, há uma mistura deles,

88
00:04:44.170 --> 00:04:48.337
então para deixar tudo limpo, apenas indente com espaços.

89
00:04:51.306 --> 00:04:54.955
Então voltando, podemos digitar self.subject

90
00:04:54.955 --> 00:04:58.425
é igual ao subject, e então vai ser basicamente isso.

91
00:04:58.425 --> 00:05:01.950
Então nós inicializamos nossa variável subject.

92
00:05:01.950 --> 00:05:05.129
O jeito que vamos definir a lógica

93
00:05:05.129 --> 00:05:09.296
aqui na função parse é se o self.subject está presente

94
00:05:10.905 --> 00:05:14.416
vamos imprimi-lo, por agora, como True.

95
00:05:14.416 --> 00:05:16.916
Senão, vamos imprimir False.

96
00:05:19.017 --> 00:05:22.635
A razão que vamos fazer isso é para testar

97
00:05:22.635 --> 00:05:25.704
se nossa lógica está funcionando corretamente. E então salve ele,

98
00:05:25.704 --> 00:05:29.408
volte para o Terminal e então rode ele novamente.

99
00:05:29.408 --> 00:05:32.708
Então isso deve imprimir a mensagem False,

100
00:05:32.708 --> 00:05:35.632
como você verá em um momento, e aqui está.

101
00:05:35.632 --> 00:05:37.840
A razão disso, claro, é porque

102
00:05:37.840 --> 00:05:40.598
nós não especificamos um subject.

103
00:05:40.598 --> 00:05:44.765
Então se nós especificarmos o subject digitando -a subject

104
00:05:46.320 --> 00:05:50.487
é igual a, qualquer coisa, foo, a mensagem vai ser True,

105
00:05:50.618 --> 00:05:52.295
como você verá em um momento.

106
00:05:52.295 --> 00:05:53.689
Então aqui está a mensagem True,

107
00:05:53.689 --> 00:05:57.856
então nossa lógica está funcionando corretamente, o que é perfeito.

108
00:05:57.961 --> 00:06:01.878
A próxima coisa que vamos fazer é buscar a URL

109
00:06:03.079 --> 00:06:06.996
se o argumento subject estiver presente.

110
00:06:07.976 --> 00:06:12.139
Para fazer isso, vamos apenas usar, por exemplo, Programming.

111
00:06:12.139 --> 00:06:14.775
Vamos para Inspect, e você verá qui

112
00:06:14.775 --> 00:06:18.903
que temos a tag &lt;a&gt; e aqui está nosso href,

113
00:06:18.903 --> 00:06:21.975
ou, na verdade, o valor que estamos procurando.

114
00:06:21.975 --> 00:06:26.142
Então se nós... há múltiplas formas de selecionar esta URL,

115
00:06:26.444 --> 00:06:30.072
sério, forma que vamos utilizar 

116
00:06:30.072 --> 00:06:32.007
é apenas usando o título.

117
00:06:32.007 --> 00:06:35.479
Então se title contém o Programming,

118
00:06:35.479 --> 00:06:39.415
vamos buscar o href para da própria tag &lt;a&gt;.

119
00:06:39.415 --> 00:06:40.791
Muito simples.

120
00:06:40.791 --> 00:06:44.247
Então para fazer isso, claro, vamos usar o Scrapy Shell.

121
00:06:44.247 --> 00:06:48.382
Então se copiarmos e voltarmos para o Terminal

122
00:06:48.382 --> 00:06:51.349
e abrir uma nova aba

123
00:06:53.383 --> 00:06:55.082
e mudar o diretório para o inicial

124
00:06:55.082 --> 00:06:57.806
e então digitar "scrapy shell".

125
00:06:57.806 --> 00:07:01.532
Vamos também maximizar esta janela,

126
00:07:01.532 --> 00:07:04.332
então "scrapy shell", dê um zoom,

127
00:07:04.332 --> 00:07:07.723
e então, em aspas simples ou duplas,

128
00:07:07.723 --> 00:07:10.572
apenas forneça a URL e pressione Enter

129
00:07:10.572 --> 00:07:11.964
e volte para o site.

130
00:07:11.964 --> 00:07:15.356
Então novamente, se houver um title

131
00:07:15.356 --> 00:07:19.165
que contém Programming, por exemplo,

132
00:07:19.165 --> 00:07:22.396
então vamos buscar a própria href.

133
00:07:22.396 --> 00:07:23.932
Muito simples.

134
00:07:23.932 --> 00:07:26.508
Então a forma que você vai buscar isso

135
00:07:26.508 --> 00:07:30.675
ou chegar no href do Programming vai ser "response.xpath"

136
00:07:32.321 --> 00:07:36.488
e então no parênteses vamos digitar as aspas simples

137
00:07:37.409 --> 00:07:40.762
e então, barras duplas, asterisco,

138
00:07:40.762 --> 00:07:44.905
e então nos colchetes vamos usar "contains"

139
00:07:44.905 --> 00:07:48.555
e então, novamente, no parênteses vamos usar title.

140
00:07:48.555 --> 00:07:51.888
Então isso é o valor do title que estamos procurando,

141
00:07:52.953 --> 00:07:56.345
então se ele contém o Programming, nós vamos, mais uma vez,

142
00:07:56.345 --> 00:07:58.601
buscar o próprio href.

143
00:07:58.601 --> 00:08:02.768
Então title, e então vírgula e então vamos digitar "Programming".

144
00:08:06.848 --> 00:08:11.015
E vamos pressionar Enter e vamos ver se está funcionando.

145
00:08:11.399 --> 00:08:14.054
Então aqui, isto está selecionando a classe do &lt;a&gt;,

146
00:08:14.054 --> 00:08:18.119
como você pode ver, nas aspas duplas há

147
00:08:18.119 --> 00:08:21.910
show-all-subjects e view-all-columns, provavelmente.

148
00:08:21.910 --> 00:08:25.590
Então se voltarmos para o site você verá

149
00:08:25.590 --> 00:08:28.326
que esse vai ser o valor correspondente,

150
00:08:28.326 --> 00:08:31.272
ou o nó HTML que estamos selecionando atualmente.

151
00:08:31.272 --> 00:08:33.738
Então a única coisa que precisamos fazer

152
00:08:33.738 --> 00:08:35.094
é buscar o próprio href

153
00:08:35.094 --> 00:08:37.622
porque não precisamos do nome da classe.

154
00:08:37.622 --> 00:08:40.892
Então para fazer isso, claro, voltamos para nosso Shell

155
00:08:40.892 --> 00:08:45.059
e então apenas use o href, como você verá em um momento.

156
00:08:46.884 --> 00:08:49.540
Então este é o dado que estamos procurando,

157
00:08:49.540 --> 00:08:53.707
então, claro, digitamos finalmente o 

158
00:08:54.677 --> 00:08:57.077
"extract_first()"

159
00:08:57.077 --> 00:09:01.244
e isso vai ser, ou isso não vai ser nossa URL final.

160
00:09:01.489 --> 00:09:05.656
Vai ser nossa URL absoluta que vamos ter que adicionar

161
00:09:05.982 --> 00:09:10.149
ou a página inicial, ou esta parte da URL

162
00:09:11.876 --> 00:09:15.636
vai ter que ser adicionada, que vamos fazer em um momento.

163
00:09:15.636 --> 00:09:18.612
Vamos copiar e colar isso como está agora

164
00:09:18.612 --> 00:09:22.779
e voltar para nosso código e definir o nome da varável como,

165
00:09:23.204 --> 00:09:25.287
por exemplo, subject_url.

166
00:09:28.059 --> 00:09:31.892
Então subject_url vai ser igual ao response.xpath

167
00:09:31.892 --> 00:09:35.620
e não vamos usar sempre o Programming

168
00:09:35.620 --> 00:09:39.188
porque o assunto vai ser adicionado dinamicamente,

169
00:09:39.188 --> 00:09:42.605
e para conseguir o subject,

170
00:09:44.568 --> 00:09:47.485
vamos colocar aqui "self.subject".

171
00:09:49.124 --> 00:09:51.700
Então isso vai, dependendo do nosso subject,

172
00:09:51.700 --> 00:09:54.804
ser adicionado dinamicamente.

173
00:09:54.804 --> 00:09:58.101
Então se mudarmos o assunto, por exemplo, para Art &amp; Design,

174
00:09:58.101 --> 00:10:01.434
vamos ter a URL para o Art &amp; Design.

175
00:10:02.546 --> 00:10:05.060
Vamos voltar para nosso código

176
00:10:05.060 --> 00:10:08.467
e finalmente vamos usar o yield no nosso Request,

177
00:10:08.467 --> 00:10:12.244
mas antes podemos apenas importar o Request.

178
00:10:12.244 --> 00:10:16.002
Então "from scrapy.http import Request"

179
00:10:19.331 --> 00:10:23.498
e então "yield Request", a URL precisa ser juntada,

180
00:10:28.451 --> 00:10:32.609
então vamos usar "response.urljoin".

181
00:10:33.231 --> 00:10:37.398
Então vamos testar isso primeiro no Scrapy Shell,

182
00:10:38.045 --> 00:10:42.212
então "response.urljoin" e então a URL

183
00:10:45.521 --> 00:10:48.353
ou a parte da URL vai ser esta aqui,

184
00:10:48.353 --> 00:10:50.881
então se pressionarmos Enter, você verá que

185
00:10:50.881 --> 00:10:53.811
esta é a URL que vai ser adicionada.

186
00:10:53.811 --> 00:10:56.753
Então se copiarmos e, vamos ver,

187
00:10:56.753 --> 00:10:59.586
colarmos... então se copiarmos aqui,

188
00:11:02.959 --> 00:11:05.709
e então esta é a URL.

189
00:11:07.206 --> 00:11:09.605
Então copie e cole agora.

190
00:11:09.605 --> 00:11:11.828
Vamos ver que esta é a URL

191
00:11:11.828 --> 00:11:13.815
correspondente ao Programming

192
00:11:13.815 --> 00:11:15.611
e o site está um pouco mais lento,

193
00:11:15.611 --> 00:11:17.892
então não se preocupe com isso.

194
00:11:17.892 --> 00:11:21.013
Então está funcionando perfeitamente bem.

195
00:11:21.013 --> 00:11:24.869
Vamos ver, então a última coisa que precisamos fazer

196
00:11:24.869 --> 00:11:29.036
é, novamente, voltar para o código... então "response.urljoin"

197
00:11:30.629 --> 00:11:34.796
e então definir o subject_url para ser a URL absoluta

198
00:11:38.168 --> 00:11:42.116
e a função callback pode ser, por exemplo,

199
00:11:42.116 --> 00:11:46.283
"self.parse_subject", então definimos a função subject.

200
00:11:50.901 --> 00:11:55.068
Então "def parse_subject(self, response)" e por enquanto

201
00:11:55.732 --> 00:11:59.815
vamos passá-lo e salvá-lo finalmente. Se nós voltarmos

202
00:12:01.108 --> 00:12:05.275
e ir para o diretório raiz e finalmente digitar

203
00:12:05.941 --> 00:12:09.364
algo como Programming no subject,

204
00:12:09.364 --> 00:12:12.324
você verá que, vamos ver,

205
00:12:12.324 --> 00:12:13.492
se tem alguma coisa faltando aqui,

206
00:12:13.492 --> 00:12:14.742
e eu esqueci,

207
00:12:16.068 --> 00:12:17.828
então deve ser isso.

208
00:12:17.828 --> 00:12:20.308
Mais uma vez, "scrapy crawl subjects -a".

209
00:12:20.308 --> 00:12:22.728
O subject vai ser definido como Programming.

210
00:12:22.728 --> 00:12:26.645
Isto, no nosso caso, vai ser esta URL,

211
00:12:26.645 --> 00:12:29.412
e vamos apenas passá-la como está.

212
00:12:29.412 --> 00:12:32.495
Então pressione Enter e aqui vemos a URL,

213
00:12:35.092 --> 00:12:38.683
foi buscada com sucesso, então tudo está funcionando corretamente.

214
00:12:38.683 --> 00:12:40.676
E para o final deste vídeo,

215
00:12:40.676 --> 00:12:42.997
vamos apenas cobrir a lógica do else.

216
00:12:42.997 --> 00:12:45.910
Então se subject_url não for fornecido, neste caso

217
00:12:45.910 --> 00:12:50.077
vamos buscar, como dito anteriormente, doze cursos diferentes.

218
00:12:55.418 --> 00:12:58.465
Para fazer isso, vá para Inspect

219
00:12:58.465 --> 00:13:02.632
e, sério, buscar essa URL não poderia ser mais simples.

220
00:13:02.682 --> 00:13:06.849
Vamos ver que na classe, ou na tag &lt;a&gt; e na classe,

221
00:13:07.821 --> 00:13:11.946
você verá que show-all-subjects view-all-courses

222
00:13:11.946 --> 00:13:13.563
será o nome da classe.

223
00:13:13.563 --> 00:13:17.725
Então se copiarmos, o jeito para testar rapidamente

224
00:13:17.725 --> 00:13:20.826
se você buscará todos eles,

225
00:13:20.826 --> 00:13:23.354
é copiar rapidamente e então colar,

226
00:13:23.354 --> 00:13:24.666
e ver se aqui nós encontramos doze instâncias

227
00:13:24.666 --> 00:13:26.426
que corresponde ao número de cursos,

228
00:13:26.426 --> 00:13:27.722
o que é perfeito.

229
00:13:27.722 --> 00:13:31.889
Então vamos para o Shell, apenas para testar e verificar,

230
00:13:32.330 --> 00:13:35.413
então response.xpath e em aspas simples

231
00:13:37.455 --> 00:13:41.386
encontre toda instância da classe que tem...

232
00:13:41.386 --> 00:13:44.526
então se copiarmos e colarmos este valor,

233
00:13:44.526 --> 00:13:46.693
então copie e cole ele,

234
00:13:47.693 --> 00:13:50.331
você verá que 12 instâncias,

235
00:13:50.331 --> 00:13:52.916
ou 12 seletores neste caso,

236
00:13:52.916 --> 00:13:56.730
vão ser selecionados, e vamos pegar apenas o href

237
00:13:56.730 --> 00:14:00.313
e do href vamos chamar o 'extract()'.

238
00:14:02.388 --> 00:14:05.555
Então se copiarmos e colarmos esta declaração

239
00:14:06.753 --> 00:14:10.586
e definir o nome de variável no else como,

240
00:14:11.585 --> 00:14:15.752
vamos ver, subjects, e vamos fornecer também o argumento,

241
00:14:18.897 --> 00:14:21.473
ou, desculpe, a mensagem log.

242
00:14:21.473 --> 00:14:25.640
Para fazer isso, nós digitamos "self.logger.info" e vamos

243
00:14:27.695 --> 00:14:31.553
imprimir "Scraping all subjects."

244
00:14:33.808 --> 00:14:36.225
E então há 12 assuntos,

245
00:14:37.744 --> 00:14:41.360
agora esses assuntos são, claro, URLs parciais.

246
00:14:41.360 --> 00:14:44.624
Então para conseguir o URL absoluto, vamos, claro, usar

247
00:14:44.624 --> 00:14:48.791
o response.urljoin, então vamos escrever um simples laço for.

248
00:14:49.424 --> 00:14:53.341
Então "for subject in subjects" então "yield Request"

249
00:15:00.750 --> 00:15:04.917
como fizemos aqui com a URL do assunto se ele for fornecido.

250
00:15:05.214 --> 00:15:09.310
Então "yield Request" e então "response.urljoin"

251
00:15:09.310 --> 00:15:12.318
e então, entre parênteses, claro,

252
00:15:12.318 --> 00:15:14.254
o subject vai ser adicionado,

253
00:15:14.254 --> 00:15:17.087
e o callback vai ser o mesmo aqui também.

254
00:15:17.087 --> 00:15:21.254
Então o callback vai ser, vamos copiar e colar isto,

255
00:15:21.342 --> 00:15:23.646
então o callback vai ser o parse_subject,

256
00:15:23.646 --> 00:15:26.126
então isso deve ser tudo.

257
00:15:26.126 --> 00:15:28.527
Salve, volte para o Terminal.

258
00:15:28.527 --> 00:15:30.862
Volte para o diretório raiz,

259
00:15:30.862 --> 00:15:35.029
e então, por enquanto não vamos fornecer o argumento,

260
00:15:35.646 --> 00:15:37.694
então isto deve imprimier

261
00:15:37.694 --> 00:15:40.830
que estamos extraindo todos os assuntos.

262
00:15:40.830 --> 00:15:42.878
Então aqui, se você ver,

263
00:15:42.878 --> 00:15:47.027
esta é a mensagem logger que definimos previamente.

264
00:15:47.777 --> 00:15:51.919
E se voltarmos para o Shell,

265
00:15:52.734 --> 00:15:56.484
vamos ver que a mensagem ou o rastreamento

266
00:15:57.470 --> 00:16:01.637
dos diferentes... todos os diferentes cursos e assuntos

267
00:16:01.886 --> 00:16:04.590
foi iniciado e funciona corretamente.

268
00:16:04.590 --> 00:16:07.057
Então isso é tudo para este vídeo.

269
00:16:07.057 --> 00:16:09.647
No próximo, nós vamos finalizar esse spider

270
00:16:09.647 --> 00:16:12.749
extraindo os nomes de curso, o nome do assunto,

271
00:16:12.749 --> 00:16:16.841
e também a URL absoluta do curso e vamos iterar

272
00:16:16.841 --> 00:16:20.957
sobre as páginas dos resultados se houver mais de 50 cursos.

273
00:16:21.057 --> 00:16:21.890
Vejo você!

