1
00:00:00,666 --> 00:00:02,851
Hey there! So today we will build a completely

2
00:00:02,851 --> 00:00:05,326
new Spider, which will crawl the entire site

3
00:00:05,326 --> 00:00:08,176
and scrape a URL from each page that is found.

4
00:00:08,176 --> 00:00:10,039
The site is this one, so,

5
00:00:10,039 --> 00:00:13,289
books.toscrape.com. And let's create our

6
00:00:15,068 --> 00:00:16,905
Scrapy spider finally.

7
00:00:16,905 --> 00:00:19,884
So let's navigate to the Desktop.

8
00:00:19,884 --> 00:00:21,407
And let's create it. So

9
00:00:21,407 --> 00:00:23,184
to do that we can type in scrapy

10
00:00:23,184 --> 00:00:26,371
startproject, and the name of the

11
00:00:26,371 --> 00:00:28,788
project can be books_crawler.

12
00:00:30,936 --> 00:00:34,246
Let's navigate here to the root directory.

13
00:00:34,246 --> 00:00:37,056
It seems like everything is set correctly.

14
00:00:37,056 --> 00:00:40,996
Let's type in now scrapy genspider,

15
00:00:40,996 --> 00:00:43,194
which will generate Spider. And let's call

16
00:00:43,194 --> 00:00:45,883
this Spider books. And then allowed_domains

17
00:00:45,883 --> 00:00:49,850
is going to be this URL.

18
00:00:52,696 --> 00:00:56,164
Let's go straight to the Spider code,

19
00:00:56,164 --> 00:01:00,045
which is located in the Spiders folder.

20
00:01:00,045 --> 00:01:02,712
What needs to be changed here is

21
00:01:03,596 --> 00:01:07,063
we should remove the www.

22
00:01:07,983 --> 00:01:11,129
The reason why is because if we don't

23
00:01:11,129 --> 00:01:14,470
do that we will encounter some errors.

24
00:01:14,470 --> 00:01:16,954
So let me actually show it to you

25
00:01:16,954 --> 00:01:20,191
in a moment so we can test it at least perfectly,

26
00:01:20,191 --> 00:01:22,880
fine the Scrapy Shell.

27
00:01:22,880 --> 00:01:25,155
So loading this page will work.

28
00:01:25,155 --> 00:01:28,596
And response.status that is as you can see is 200.

29
00:01:28,596 --> 00:01:32,096
But if we add the www. part in the URL,

30
00:01:33,172 --> 00:01:37,016
as you will see there is twisted.internet.error.

31
00:01:37,016 --> 00:01:40,239
So that will definitely cause issues.

32
00:01:40,239 --> 00:01:43,381
Let see, I haven't exited correctly

33
00:01:43,381 --> 00:01:47,381
the Terminal, um so just make sure that www.

34
00:01:47,381 --> 00:01:49,545
once again part is not present

35
00:01:49,545 --> 00:01:52,487
in the start_urls, and that allowed_domains

36
00:01:52,487 --> 00:01:55,287
is the correct URL.

37
00:01:56,622 --> 00:01:59,818
And let's see what else we can do.

38
00:01:59,818 --> 00:02:02,379
So pretty much the first thing that we

39
00:02:02,379 --> 00:02:05,604
need to do is remove the Spider,

40
00:02:06,260 --> 00:02:08,454
or the import for the Spider.

41
00:02:08,454 --> 00:02:11,346
We will not use the default Spider,

42
00:02:11,346 --> 00:02:13,953
we will use CrawlSpider.

43
00:02:13,953 --> 00:02:16,392
Now CrawlSpider is going to to have some

44
00:02:16,392 --> 00:02:19,309
other attributes and reserved names

45
00:02:20,182 --> 00:02:21,505
and stuff like that, just like the

46
00:02:21,505 --> 00:02:24,447
Spider had it. And the one of them will

47
00:02:24,447 --> 00:02:27,222
be the Rules, as you will see in a moment.

48
00:02:27,222 --> 00:02:30,972
But let's first import it, so from

49
00:02:31,983 --> 00:02:36,099
scrapy.spiders import CrawlSpider.

50
00:02:36,827 --> 00:02:40,244
And we will also grab the Rule sub module.

51
00:02:42,113 --> 00:02:45,530
Just like with the Spider, so pretty much

52
00:02:46,583 --> 00:02:49,067
the Spider as I'm referring to Spider,

53
00:02:49,067 --> 00:02:51,461
I mean Spider from the previous videos,

54
00:02:51,461 --> 00:02:53,945
that we used CrawlSpider that has alongside

55
00:02:53,945 --> 00:02:55,359
with the name, allowed_domains,

56
00:02:55,359 --> 00:02:58,460
start_urls, reserved words, also it has the

57
00:02:58,460 --> 00:03:01,867
rules variable. Now rules variable is going

58
00:03:01,867 --> 00:03:04,673
to set one or more Rule really.

59
00:03:04,673 --> 00:03:08,739
Simple as that. And in the first Rule

60
00:03:08,925 --> 00:03:12,072
and only Rule that we will collect or get

61
00:03:12,072 --> 00:03:16,172
is going to have object called LinkExtractor().

62
00:03:18,076 --> 00:03:21,205
This object is pretty much going to

63
00:03:21,205 --> 00:03:23,321
tell us that once we are in

64
00:03:23,321 --> 00:03:26,014
some specific page we would like to grab

65
00:03:26,014 --> 00:03:29,405
all of the URLs, no matter how many of them

66
00:03:29,405 --> 00:03:33,572
there are, and then we can select some other

67
00:03:33,894 --> 00:03:38,060
attributes and parse different type

68
00:03:38,662 --> 00:03:40,439
of pages and stuff like that.

69
00:03:40,439 --> 00:03:43,957
So let's collect or get the two other

70
00:03:43,957 --> 00:03:46,614
arguments that we will use. So the first one

71
00:03:46,614 --> 00:03:49,003
is going to be callback. Now previously

72
00:03:49,003 --> 00:03:53,086
we talked or called, better said,

73
00:03:53,622 --> 00:03:57,145
callback would be self.parse.

74
00:03:57,145 --> 00:04:00,536
So this would work but by syntax

75
00:04:00,536 --> 00:04:03,546
in Python at least, since this is variable name,

76
00:04:03,546 --> 00:04:05,813
this will not work, so it has to have

77
00:04:05,813 --> 00:04:09,955
the single or double quotes surrounding it

78
00:04:11,103 --> 00:04:14,290
and also self, we don't need it,

79
00:04:14,290 --> 00:04:18,140
because it's in the same sort of

80
00:04:18,542 --> 00:04:20,655
layer so to speak.

81
00:04:20,655 --> 00:04:23,572
And finally the other thing is that

82
00:04:24,499 --> 00:04:26,730
the parse function name is

83
00:04:26,730 --> 00:04:29,948
reserved just for the Spider.

84
00:04:29,948 --> 00:04:32,637
The default Spider that Scrapy uses

85
00:04:32,637 --> 00:04:34,912
so we can use any other function

86
00:04:34,912 --> 00:04:38,398
variable names such as, parse_page.

87
00:04:38,398 --> 00:04:40,429
The last attribute that we will use

88
00:04:40,429 --> 00:04:44,446
is called follow. Follow is going to be

89
00:04:44,446 --> 00:04:48,613
either True or False, so boolean expression.

90
00:04:48,902 --> 00:04:51,872
And if set to True, it will once it goes to

91
00:04:51,872 --> 00:04:55,622
the start_urls, or once it scrapes this page,

92
00:04:56,750 --> 00:04:59,587
LinkExtractor() will collect all of the URLs

93
00:04:59,587 --> 00:05:02,103
present in this page. And if follow is

94
00:05:02,103 --> 00:05:04,964
set to True, it will for example collect

95
00:05:04,964 --> 00:05:08,319
this URL and then this URL it will collect

96
00:05:08,319 --> 00:05:11,166
the books URL and go to the next page,

97
00:05:11,166 --> 00:05:13,936
collect all of the categories if it's

98
00:05:13,936 --> 00:05:16,955
not yet scraped, and go to the next page

99
00:05:16,955 --> 00:05:18,696
and stuff like that by default,

100
00:05:18,696 --> 00:05:20,890
if I haven't mentioned already,

101
00:05:20,890 --> 00:05:24,031
Scrapy will filter out, duplicate Requests.

102
00:05:24,031 --> 00:05:28,198
So if this type of pages or category related pages

103
00:05:29,040 --> 00:05:33,123
are scraped once, they will not be scraped twice.

104
00:05:34,910 --> 00:05:38,307
So let's see if set to False, as you

105
00:05:38,307 --> 00:05:40,089
will see in a moment, the Spider

106
00:05:40,089 --> 00:05:43,089
is just going to go to the homepage,

107
00:05:44,509 --> 00:05:47,651
process the links and pretty much stop

108
00:05:47,651 --> 00:05:50,498
and not work, so, let's actually save it.

109
00:05:50,498 --> 00:05:53,498
So let's see if everything is in the

110
00:05:54,786 --> 00:05:58,953
correct order, so I haven't added LinkExtractor,

111
00:05:59,351 --> 00:06:00,976
LinkExtractor is

112
00:06:02,626 --> 00:06:06,759
from scrapy.linkextractors import LinkExtractor.

113
00:06:08,355 --> 00:06:10,676
Save it and let's actually run this.

114
00:06:10,676 --> 00:06:12,759
So let's see if it works.

115
00:06:15,328 --> 00:06:18,942
And it seems like I haven't done something

116
00:06:18,942 --> 00:06:22,275
correctly so lets see what is the issue.

117
00:06:23,285 --> 00:06:26,702
So it's invalid syntax, and it seems like

118
00:06:29,736 --> 00:06:33,005
this should work, so let's see. Perfect.

119
00:06:33,005 --> 00:06:36,944
So 75 as you can see, responses, are generated,

120
00:06:36,944 --> 00:06:40,944
or better said 74, with the 200 response status,

121
00:06:41,858 --> 00:06:44,338
and 1 is for the 404s. So let's see,

122
00:06:44,338 --> 00:06:47,770
I think that is the robots.txt? Perfect.

123
00:06:47,770 --> 00:06:51,936
So it seems like Scrapy in this type of,

124
00:06:51,936 --> 00:06:55,019
or this version at least, is going to

125
00:06:56,324 --> 00:06:59,741
first page that is going to, that it's

126
00:07:00,989 --> 00:07:03,097
going to go to, is going to be

127
00:07:03,097 --> 00:07:04,956
robots.txt type of page,

128
00:07:04,956 --> 00:07:09,123
and since this type or site doesn't have them,

129
00:07:09,289 --> 00:07:12,884
so let's copy and paste it, to verify it,

130
00:07:12,884 --> 00:07:15,033
as you can see 404, so it's not found,

131
00:07:15,033 --> 00:07:17,507
so this site doesn't have the, or doesn't

132
00:07:17,507 --> 00:07:21,674
contain the 404 pages, or sorry robots type of pages,

133
00:07:23,611 --> 00:07:26,789
this will not be of course processed.

134
00:07:26,789 --> 00:07:30,372
So let's see what else we can do with this.

135
00:07:33,735 --> 00:07:36,833
Let's see, from Terminal, let's try to run it

136
00:07:36,833 --> 00:07:40,800
once again and see. 74 only pages are successful.

137
00:07:40,800 --> 00:07:43,800
Perfect. So there are not any errors,

138
00:07:43,800 --> 00:07:49,000
which is perfect also. And now let's talk
about the LinkExtractor arguments.

139
00:07:49,000 --> 00:07:56,000
One of them is deny_domains. And let's
type in, in the parentheses, domain name,

140
00:07:56,000 --> 00:08:03,000
such as google.com. If we save this, and
then just scrape it as it is, you will see

141
00:08:03,000 --> 00:08:08,800
that there isn't really anything changed.
This is because the books.toscrape.com

142
00:08:08,800 --> 00:08:13,400
will not contain any Google-related
pages, or better said, domain names,

143
00:08:13,400 --> 00:08:19,000
such as google.com. But this is an especially
good idea if you know that the site actually has

144
00:08:19,000 --> 00:08:24,100
this type of related pages, because
then you will get potentially banned or

145
00:08:24,100 --> 00:08:29,200
thrown CAPTCHA with Google. And the
spider will actually take longer to complete.

146
00:08:29,200 --> 00:08:35,000
Also, some of the pages or domains
that you would like to avoid will be,

147
00:08:35,000 --> 00:08:38,500
for example, social media accounts
such as Facebook, Instagram,

148
00:08:38,500 --> 00:08:42,800
Google+, Twitter, LinkedIn, etc.
because they are really restrictive

149
00:08:42,800 --> 00:08:49,000
when it comes to scraping. And spider,
once again, or better said, a crawler will be faster

150
00:08:49,000 --> 00:08:54,500
without scraping the necessary,
or unnecessary, pages.

151
00:08:54,500 --> 00:09:00,600
Another argument that
LinkExtractor provides is called allow.

152
00:09:00,600 --> 00:09:07,400
So this one, for example, let's say we have a
task to only scrape the music-related pages.

153
00:09:07,400 --> 00:09:13,200
So we will just type in here music,
and then save it. So this will process

154
00:09:13,200 --> 00:09:20,400
the pages that have the music in the URL,
such as this one. So it will actually go

155
00:09:20,400 --> 00:09:26,700
to the music-related category, and then
scrape only the URLs that have music in them.

156
00:09:26,700 --> 00:09:31,800
So, for example, this page will be
scraped. And if we run it, you will see

157
00:09:31,800 --> 00:09:39,000
that it's going to be extremely fast because
there isn't a lot of pages to be processed.

158
00:09:39,000 --> 00:09:43,000
And, as you can see, it's already
finished. We already got some

159
00:09:43,000 --> 00:09:48,900
filtered out results from the category.
And, yeah, that will be pretty much it

160
00:09:48,900 --> 00:09:55,500
for this video. So, as you can see, in around
16 or so lines we built the entire crawler

161
00:09:55,500 --> 00:10:00,000
that you can use to, pretty much,
index the entire domain.

162
00:10:00,000 --> 00:10:04,500
So if we want to index
every URL from some domain,

163
00:10:04,500 --> 00:10:11,600
you will type in yield URL, and then just
response.url. Some of the... another use case

164
00:10:11,600 --> 00:10:17,800
for this simple scraper or crawler
can be for extracting email addresses

165
00:10:17,800 --> 00:10:26,000
from entire domains. So you can fetch all of
the different emails or scrape forums, etc.

166
00:10:26,000 --> 00:10:28,700
Thanks for watching.

