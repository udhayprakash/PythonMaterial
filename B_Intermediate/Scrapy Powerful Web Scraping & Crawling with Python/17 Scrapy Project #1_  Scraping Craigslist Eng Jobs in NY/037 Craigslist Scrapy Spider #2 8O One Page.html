<html>
                        <head>
                        <link href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">
                        <title>037 Craigslist Scrapy Spider #2 8O One Page</title>
                        </head>
                        <body>
                        <div class="container">
                        <div class="row">
                        <div class="col-md-10 col-md-offset-1">
                            <p class="lead"><div class="asset-container">
    <div class="asset-container__padding article-view">
        <div class="w3c-default">
            <p>In the second part of this Scrapy tutorial, we will scrape the details of <a href="https://newyork.craigslist.org/search/egr" rel="nofollow" target="_blank">Craigslists Architecture &amp; Engineering jobs in New York</a>. For now, you will start by only one page. In the third part of the tutorial, you will learn how to navigate to next pages.</p>

<p>Before starting this Scrapy exercise, it is very important to understand the main approach:<br></p>



































<p><br></p>



































<h4>The Secret: Wrapper</h4>



































<p>In the first part of this Scrapy tutorial, we extracted titles only. However, if you want to scrape several details about each job, you will not extract them separately, and then loop on each of them. No! Actually, you scrape the whole container or wrapper of each job including all the information you need, and then extract pieces of information from each container/wrapper.</p>





































<p>To see how this container/wrapper looks like, right-click any job on the Craigslists page and select Inspect; you will see this:</p>



































<figure><img src="https://udemy-images.s3.amazonaws.com:443/redactor/2017-05-18_09-18-35-f1cca636a8e7dc97ac8771107835a8af/Craigslist-wrapper-code.png"></figure>



































<p><br></p>

















<p>As you can see, each result is inside an HTML list 
			&lt;li&gt;tag.</p>



































<figure><img src="https://udemy-images.s3.amazonaws.com:443/redactor/2017-05-18_09-19-30-401e9102582e72e16e72fa29ed2f3e45/Craigslist-wrapper-code.png"></figure>



































<p><br></p>

















<p>If you expand the &lt;li&gt; tag, you will see this HTML code:</p>



































<pre class="prettyprint linenums">&lt;li class="result-row" data-pid="6112478644"&gt;
    &lt;a href="/brk/egr/6112478644.html" class="result-image gallery empty"&gt;&lt;/a&gt;
    &lt;p class="result-info"&gt;
        &lt;span class="icon icon-star" role="button"&gt;
            &lt;span class="screen-reader-text"&gt;favorite this post&lt;/span&gt;
        &lt;/span&gt;
        &lt;time class="result-date" datetime="2017-05-01 12:35" title="Mon 01 May 12:35:41 PM"&gt;May 1&lt;/time&gt;
        &lt;a href="/brk/egr/6112478644.html" data-id="6112478644" class="result-title hdrlnk"&gt;Project Architect&lt;/a&gt;
        &lt;span class="result-meta"&gt;
            &lt;span class="result-hood"&gt; (Brooklyn)&lt;/span&gt;
            &lt;span class="result-tags"&gt;
                &lt;span class="maptag" data-pid="6112478644"&gt;map&lt;/span&gt;
            &lt;/span&gt;
            &lt;span class="banish icon icon-trash" role="button"&gt;
                &lt;span class="screen-reader-text"&gt;hide this posting&lt;/span&gt;
            &lt;/span&gt;
            &lt;span class="unbanish icon icon-trash red" role="button" aria-hidden="true"&gt;&lt;/span&gt;
            &lt;a href="#" class="restore-link"&gt;
                &lt;span class="restore-narrow-text"&gt;restore&lt;/span&gt;
                &lt;span class="restore-wide-text"&gt;restore this posting&lt;/span&gt;
            &lt;/a&gt;
        &lt;/span&gt;
    &lt;/p&gt;
&lt;/li&gt;</pre>



































































<p>As you can see, the &lt;li&gt; tag includes all the information you need; so you can consider it your wrapper. Actually, you can even start from the &lt;p&gt; tag which includes the same information you need. The &lt;li&gt; is distinguished by the class result-row while the &lt;p&gt; is distinguished by the class result-info so each of them is unique, and can be easily distinguished by your XPath expression.</p>



































<p>Note: If you are using the same spider from the Basic Scrapy Spider, delete any code under the parse() function, and start over, or just copy the file into the same spiders folder and change name = "jobs" in the basic spider to anything else like <code>name = "jobs-titles"</code> and keep the new one <code>name = "jobs"</code> as is.</p>

































<p><br></p>

































<h4>Extracting All Wrappers</h4>



































<p>As we agreed, you first need to scrape all the wrappers from the page. So under the parse() function, write the following:</p>

































<p><code>jobs = response.xpath('//p[@class="result-info"]')</code><br></p>































<p><br></p>































<p>Note that here you will not use <code>extract()</code> because it is the wrapper from which you will extract other HTML nodes.</p>































<h4><br>Extracting Job Titles</h4>



































<p>You can extract the job titles from the wrappers using a for loop as follows:</p>































<pre class="prettyprint linenums">for job in jobs:
    title = job.xpath('a/text()').extract_first()
    yield{'Title':title}</pre>































































<p><br>The first observation is that in the for loop, you do not use response (which you already used to extract the wrapper). Instead, you use the wrapper selector which are referred to as job.</p>



































<p>Also, as you can see, we started the XPath expression of jobs by <code>//</code> meaning it starts from &lt;html&gt; until this &lt;p&gt; whose class name is result-info.</p>



































<p>However, we started the XPath expression of title without any slashes, because it complements or depends on the XPath expression of the job wrapper. If you rather want to use slashes, you will have to precede it with a dot to refer to the current node as follows:</p>





























<p><code>title = job.xpath('.//a/text()').extract_first()</code><br></p>





























<p>As we explained in the first part of this Scrapy tutorial, a refers to the first &lt;a&gt; tag inside the &lt;p&gt; tag, and <code>text()</code> refers to the text inside the &lt;a&gt; tag which is the job title.</p>



































<p>Here, we are using <code>extract_first()</code> because in each iteration of the loop, we are in a wrapper with only one job.</p>



































<p>What is the difference between the above code and what we had in the first part of the tutorial? So far, this will give the same result. However, as you want to extract and yield more than one element from the wrapper, this approach is more straightforward because now, you can extract other elements like the address and URL of each job.</p>



























<h4><br>Extracting Job Addresses and URLs</h4>



































<p>You can extract the job address and URL from the wrappers using the same for loop as follows:<br></p>



























<pre class="prettyprint linenums">for job in jobs:
    title = job.xpath('a/text()').extract_first()
    address = job.xpath('span[@class="result-meta"]/span[@class="result-hood"]/text()').extract_first("")[2:-1]
    relative_url = job.xpath('a/@href').extract_first()
    absolute_url = response.urljoin(relative_url)

    yield{'URL':absolute_url, 'Title':title, 'Address':address}</pre>























































<p><br></p>



































<p>To extract the job address, you refer to the &lt;span&gt; tag whose class name is result-meta and then the &lt;span&gt; tag whose class name is result-hood and then the text() in it. The address is between brackets like (Brooklyn); so if you want to delete them, you can use string slicing [2:-1]. However, this string slicing will not work if there is no address (which is the case for some jobs) because the value will be None which is not a string! So you have to add empty quotes inside extract_first("") which means if there is no result, the result is .</p>



































<p>To extract the job URL, you refer to the &lt;a&gt; tag and the value of the href attribute, which is the URL. Yes, <code>@</code> means an attribute.</p>



































<p>However, this is a relative URL, which looks like: <em><strong>/brk/egr/6112478644.html</strong></em> so, to get the absolute URL to be able to use it later, you can either use Python concatenation as follows:</p>

























<p><code>absolute_url = "https://newyork.craigslist.org" + relative_url</code><br><br></p>



































<p><strong>Note:</strong> Concatenation is another case in which you need to add quotes to <code>extract_first("")</code> because concatenation works only on strings and cannot work on None values.</p>



































<p>Otherwise, you can simply use the <code>urljoin()</code> method, which builds a full absolute URL:</p>























<p><code>absolute_url = response.urljoin(relative_url)</code><br><br></p>



































<p>Finally, yield your data using a dictionary:</p>





















<p><code>yield{'URL':absolute_url, 'Title':title, 'Address':address}</code><br><br></p>







































<h4>Running the Spider and Storing Data</h4>



































<p>Just as we did in the first part of this Scrapy tutorial, you can use this command to run your spider and store the scraped data to a CSV file.</p>



















<p><code>scrapy crawl jobs -o result-jobs-one-page.csv</code></p>



















<p><br></p>



















<p><br></p>
        </div>
    </div>
</div>
</p>
                        </div>
                        </div>
                        </div>
                        <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script>
                        </body>
                        </html>