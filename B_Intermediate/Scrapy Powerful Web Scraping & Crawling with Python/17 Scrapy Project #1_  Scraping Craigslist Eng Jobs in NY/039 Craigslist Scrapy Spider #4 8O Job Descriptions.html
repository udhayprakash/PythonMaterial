<html>
                        <head>
                        <link href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">
                        <title>039 Craigslist Scrapy Spider #4 8O Job Descriptions</title>
                        </head>
                        <body>
                        <div class="container">
                        <div class="row">
                        <div class="col-md-10 col-md-offset-1">
                            <p class="lead"><div class="asset-container">
    <div class="asset-container__padding article-view">
        <div class="w3c-default">
            <p>In this spider, we will open the URL of each job and scrape its description. The code we wrote so far will change; so if you like, you can copy the file into the same spiders folder, and change the spider name of the previous one to be something like <code>name = "jobsall"</code> and you can keep the new file <code>name = "jobs"</code> as is.</p>

<h4><br>Passing the Data to a Second Function</h4>





















<p>In the parse() function, we had the following yield in the for loop:</p>























<p><code>yield{'URL':absolute_url, 'Title':title, 'Address':address}</code><br><br></p>















































<p>We yielded the data at this stage because we were scraping the details of each job from one page, but now you need to create a new function called for example <code>parse_page()</code> to open the URL of each job and scrape the job description from each job dedicated page; so you have to pass the URL of the job from the <code>parse()</code> function to the <code>parse_page()</code> function in a callback using the <code>Request()</code> method as follows:</p>

























<p><code>yield Request(absolute_url, callback=self.parse_page)</code><br></p>















































<p>What about the data you have already extracted? You need to pass those values of titles and addresses from the <code>parse()</code> function to the <code>parse_page()</code> function as well, using meta in a dictionary as follows:</p>



























<p><code>yield Request(absolute_url, callback=self.parse_page, meta={'URL': absolute_url, 'Title': title, 'Address':address})</code><br></p>















































<h4>Function to Extract Job Descriptions</h4>















































<p>So the <code>parse_page()</code> function will be as follows:</p>































<pre class="prettyprint linenums">def parse_page(self, response):
    url = response.meta.get('URL')
    title = response.meta.get('Title')
    address = response.meta.get('Address')
    description = "".join(line for line in response.xpath('//*[@id="postingbody"]/text()').extract())
    yield{'URL': url, 'Title': title, 'Address':address, 'Description':description}</pre>































































<p><br></p>















































<p>Here, you should deal with meta as a dictionary and assign each value to a new variable. To get the value of each key in the dictionary, you can use the Pythons dictionary <code>get()</code> method as usual.</p>















































<p>Otherwise, you can use the other way around by adding the new value description to the meta dictionary and then simply yield meta to retrieve all the data as follows:</p>

































<pre class="prettyprint linenums">def parse_page(self, response):
    response.meta['Description'] = description
    yield response.meta</pre>



































































<p><br></p>















































<p>Note that a job description might be in more than one paragraph; so we used <code>join()</code> to merge them.</p>















































<p>Now, we can also extract compensation and employment type. They are in a &lt;p&gt; tag whose class name is attrgroup and then each of them is in a &lt;span&gt; tag, but with no class or id to distinguish them.</p>



































<pre class="prettyprint linenums">&lt;p class="attrgroup"&gt;
    &lt;span&gt;compensation: &lt;b&gt;To 110K, DOE&lt;/b&gt;&lt;/span&gt;&lt;br&gt;
    &lt;span&gt;employment type: &lt;b&gt;full-time&lt;/b&gt;&lt;/span&gt;&lt;br&gt;
&lt;/p&gt;
 </pre>



















































































<p>So we can use span[1] and span[2] to refer to the first &lt;span&gt; tag of compensation and the second &lt;span&gt; tag of employment type respectively.</p>





































<pre class="prettyprint linenums">compensation = response.xpath('//p[@class="attrgroup"]/span[1]/b/text()').extract_first()
employment_type = response.xpath('//p[@class="attrgroup"]/span[2]/b/text()').extract_first()</pre>





















































































<p>Actually, this is the same as if you use list slicing. In this case, you will use extract() not extract_first() and follow the expression with [0] or [1] which can be added directly after the expression as shown below or even after <code>extract()</code>. Note that in the previous way, we counted the tags from [1] while in this way, we are counting from [0] just as any list.<br></p>







































<pre class="prettyprint linenums">compensation = response.xpath('//p[@class="attrgroup"]/span/b/text()')[0].extract()
employment_type = response.xpath('//p[@class="attrgroup"]/span/b/text()')[1].extract()</pre>



















































































<h4></h4><h4>Storing Scrapy Output Data to CSV, XML or JSON</h4>















































<p>You can run the spider and save the scraped data to either CSV, XML or JSON as follows:</p>













































<p><code>$ scrapy crawl jobs -o result-jobs-multi-pages-content.csv</code></p>













































<p><code>$ scrapy crawl jobs -o result-jobs-multi-pages-content.xml</code></p>













































<p><code>$ scrapy crawl jobs -o result-jobs-multi-pages-content.json</code></p>













































<p><br></p>
        </div>
    </div>
</div>
</p>
                        </div>
                        </div>
                        </div>
                        <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script>
                        </body>
                        </html>