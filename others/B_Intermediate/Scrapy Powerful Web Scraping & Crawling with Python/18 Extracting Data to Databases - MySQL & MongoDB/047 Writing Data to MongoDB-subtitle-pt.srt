1
00:00:00,330 --> 00:00:07,530
Olá! Entã hoje vamos aprender como escrever
os dados que foram extraídos para o MongoDB. O spider que vamos

2
00:00:07,530 --> 00:00:09,620
reutilizar é o books_crawler.

3
00:00:09,660 --> 00:00:16,080
Este é um spider bem conhecido, que vai para
a página inicial do books.toscrape.com e então processa

4
00:00:16,590 --> 00:00:25,870
cada um dos livros e olha as páginas
e ele extrai, ou pega o callback para o parse_book

5
00:00:25,930 --> 00:00:33,660
pela URL do livro, e então de cada URL de livro,
ele vai extrair sete pontos de dados

6
00:00:33,660 --> 00:00:36,270
diferentes que estou destacando agora.

7
00:00:36,270 --> 00:00:44,040
Eu vou deletar esta lógica para extrair ou rastrear
o site inteiro porque isto é mais um exemplo

8
00:00:44,040 --> 00:00:48,460
de MongoDB como um database do que de scraping.

9
00:00:48,750 --> 00:00:55,870
Então a primeira coisa que precisamos fazer é digitar
para instalar uma dependência que vamos usar, então

10
00:00:55,920 --> 00:00:59,770
para fazer isso, podemos digitar "sudo pip install pymongo".

11
00:00:59,820 --> 00:01:05,010
Eu já fiz isso então eu vou receber
que os requisitos já estão satisfeitos.

12
00:01:05,010 --> 00:01:13,620
E a segunda coisa que precisamos fazer é ir para o settings.py
e então navegar para o ITEM_PIPELINES,

13
00:01:13,740 --> 00:01:26,130
que vão estar aqui, descomente eles
e então escreva, por exemplo, "MongoDBPipeline" que

14
00:01:26,130 --> 00:01:33,350
vai para o pipelines.py
e então defina a classe MongoDBPipeline em um momento.

15
00:01:33,600 --> 00:01:39,530
E a segunda coisa que precisamos fazer
é definir atributos MongoDB diferentes.

16
00:01:39,540 --> 00:01:47,450
Um deles vai ser "MONGODB_SERVER",
que vai ser "localhost".

17
00:01:47,460 --> 00:01:57,210
A segunda coisa que precisamos atribuir é
a porta, que vai ser "27017".

18
00:01:57,210 --> 00:02:03,900
A terceira coisa é "MONGODB_DB" ou o nome do database,
que vai ser, vamos chamar ele de "books".

19
00:02:04,400 --> 00:02:11,550
E a quarta ou a última coisa que precisamos definir
é o "MONGODB_COLLECTION", o nome da coleção

20
00:02:11,590 --> 00:02:18,360
vai ser, por exemplo, "products".
A coleção, novamente, quando eu me referir a coleção isso basicamente

21
00:02:18,360 --> 00:02:24,210
quer dizer a tabela do MySQL.
Salve, então volte

22
00:02:24,240 --> 00:02:27,020
e então finalmente vá para o Pipelines.

23
00:02:27,250 --> 00:02:34,090
Aqui podemos deletar isto
e então vamos digitar "from pymongo

24
00:02:34,420 --> 00:02:37,900
import MongoClient".

25
00:02:37,980 --> 00:02:46,170
Segunda coisa que precisamos fazer é importar as configurações do Scrapy.
Para fazer isso, pode digitar "from scrapy.conf import settings"

26
00:02:46,570 --> 00:02:57,810
Então definir a "class", que vai ser
"MongoDBPipeline". Esta é a mesma classe que aqui,

27
00:02:58,500 --> 00:03:04,320
já que ele vai para o pipelines.py
e então MongoDBPipeline.

28
00:03:04,320 --> 00:03:11,940
E então nos atributos, ou parâmetros,
podemos atribuir ele como um "object".

29
00:03:12,300 --> 00:03:16,530
Vamos inicializar parâmetros diferentes do settings.py.

30
00:03:16,530 --> 00:03:20,620
"self" é o parâmetro.

31
00:03:20,670 --> 00:03:27,670
A primeira coisa que podemos atribuir é o "connection",
que vai ser "MongoClient", então ele vai herdar

32
00:03:27,670 --> 00:03:40,400
do pymongo, e aqui vamos para "settings['MONGODB_SERVER']",
que vai para o settings.py

33
00:03:40,400 --> 00:03:47,300
e então vai para o MONGODB_SERVER,
que vai ser definido para o localhost.

34
00:03:47,300 --> 00:03:51,710
A segunda coisa que precisa ser feita é, novamente, "settings".

35
00:03:56,510 --> 00:03:56,860
Ok.

36
00:03:56,890 --> 00:03:58,600
Isto é mais parecido com isso.

37
00:03:58,930 --> 00:04:08,350
E então "MONGODB_PORT", que vai ser este número
e vamos ver o que mais falta

38
00:04:08,350 --> 00:04:08,990
ser feito.

39
00:04:09,040 --> 00:04:15,000
Então a segunda coisa que precisa ser feita aqui é
definir o database, que vai para o settings

40
00:04:15,550 --> 00:04:22,360
e vai herdar do...
ou vai ser chamado books, então para fazer isso, digitamos

41
00:04:22,360 --> 00:04:30,960
"db = connection", e então vamos ver,
nos colchetes podemos digitar

42
00:04:32,020 --> 00:04:33,260
"settings['MONGODB_DB']"

43
00:04:37,470 --> 00:04:38,430
Está certo.

44
00:04:38,750 --> 00:04:39,380
OK.

45
00:04:39,390 --> 00:04:47,670
E então finalmente "self.collection",
self vai ser necessário porque vamos digitar, em um

46
00:04:47,670 --> 00:04:55,690
momento, outra função
e "self.collection = db[]".

47
00:04:56,030 --> 00:05:02,930
Vamos ver o settings, então "settings['MONGODB_COLLECTION']".

48
00:05:03,000 --> 00:05:10,610
Finalmente vamos definir outra função,
que vai ser o "process_item", e essa função

49
00:05:10,610 --> 00:05:12,030
terá

50
00:05:12,060 --> 00:05:21,320
self, item, spider, isto também é
uma função bem conhecida que já cobrimos.

51
00:05:22,360 --> 00:05:32,920
E então digitamos "self.collection.insert(dict(item))"

52
00:05:32,920 --> 00:05:37,660
e então fechamos, novamente, parênteses.

53
00:05:37,680 --> 00:05:41,970
E então finalmente "return item". Vamos salvar isso e

54
00:05:42,230 --> 00:05:46,890
vamos ver se há alguma mistura de tabs e espaços em algum lugar.

55
00:05:47,060 --> 00:05:56,430
Não há, então esta linha vai inserir
os dados no database Mongo, auto-explicativo.

56
00:05:56,430 --> 00:05:58,480
E se rodarmos ele.

57
00:05:58,500 --> 00:06:00,840
Então voltamos para o diretório raiz,

58
00:06:01,370 --> 00:06:04,760
abrimos o Terminal, vamos maximizá-lo,

59
00:06:04,820 --> 00:06:06,020
"scrapy crawl books".

60
00:06:06,060 --> 00:06:13,020
E não queremos atribuir nenhum outro atributo
e coisas como essa para ter isto escrito no database Mongo.

61
00:06:13,020 --> 00:06:16,170
Então vamos pressionar Enter e ver.

62
00:06:16,200 --> 00:06:17,580
Então 20 ou mais.

63
00:06:17,640 --> 00:06:20,210
É, exatamente 20 livros foram extraídos.

64
00:06:20,580 --> 00:06:24,120
E vamos para o mongo e vamos ver.

65
00:06:24,160 --> 00:06:29,000
"show dbs", eu não criei nenhum outro database.

66
00:06:29,010 --> 00:06:34,200
E como você pode ver o database book do settings.py foi criado.

67
00:06:34,200 --> 00:06:44,250
Então se irmos e escrevermos "use books",
e então "show collections", vamos ver

68
00:06:44,250 --> 00:06:47,250
os produtos listados.

69
00:06:47,310 --> 00:06:47,900
Perfeito.

70
00:06:47,940 --> 00:06:54,220
E se digitarmos agora, vamos ver, "db.collection"

71
00:06:57,110 --> 00:07:02,980
não collections, desculpe, "products.find();".

72
00:07:03,130 --> 00:07:03,860
Perfeito.

73
00:07:04,140 --> 00:07:11,170
E se novamente formos e usar pretty(),
por exemplo, está mais legível, perfeito.

74
00:07:11,190 --> 00:07:16,280
E como você pode ver, temos nossos dados escritos no database.

75
00:07:16,350 --> 00:07:24,330
Então se quisermos ir para outro lote,
podemos apenas rodar o spider como está ou apenas fechar ele

76
00:07:24,990 --> 00:07:27,430
e então rastrear mais uma vez.

77
00:07:27,600 --> 00:07:32,780
E a forma que testamos isso é, vamos ver.

78
00:07:32,910 --> 00:07:34,470
Então "show collections".

79
00:07:34,470 --> 00:07:35,400
Desculpe,

80
00:07:35,550 --> 00:07:46,110
"use books"

81
00:07:46,110 --> 00:07:49,830
"db.products.find().pretty();"

82
00:07:50,100 --> 00:08:03,950
E aqui, se deletarmos ele ou, vamos ver,
ele deve estar em algum lugar 40 livros, e aqui se digitarmos,

83
00:08:04,190 --> 00:08:15,490
vamos ver, "db" e se minha memória está correta
então podemos digitar "db.products

84
00:08:15,500 --> 00:08:21,180
.remove({});".

85
00:08:21,230 --> 00:08:28,970
Vamos ver que devem ter sido removidos 40 produtos
porque já extraímos o site duas vezes, ou

86
00:08:28,970 --> 00:08:37,190
a página inicial duas vezes.
Perfeito, então há 40 produtos removidos ou nós, neste caso,

87
00:08:37,190 --> 00:08:37,810
como este aqui.

88
00:08:38,000 --> 00:08:40,110
E isso deve ser tudo para este vídeo.

89
00:08:40,140 --> 00:08:41,050
E obrigado por assistir!

