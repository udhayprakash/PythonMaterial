WEBVTT

00:00.330 --> 00:07.530
Hi there! So today we are going to learn how to
write data that is scraped to MongoDB, the spider that we will

00:07.530 --> 00:09.620
reuse is books_crawler.

00:09.660 --> 00:16.080
This is a well known spider that will go to
the home page of the books.toscrape.com and then process

00:16.590 --> 00:25.870
each and every book and looks to the page
and it will scrape or get the callback to the parse_book

00:25.930 --> 00:33.660
by the book URL, and then from the each book URL,
it will scrape seven different data

00:33.660 --> 00:36.270
points that I'm highlighting right now.

00:36.270 --> 00:44.040
I'm going to delete this logic for scraping or crawling
the entire site because this is just an example

00:44.040 --> 00:48.460
more of MongoDB as a database not as a scraping.

00:48.750 --> 00:55.870
So the first thing that we need to do is type in to
install a dependency that we will have to use so

00:55.920 --> 00:59.770
to do that we can type in "sudo pip install pymongo".

00:59.820 --> 01:05.010
I already have this so I will get
the requirement is already satisfied.

01:05.010 --> 01:13.620
And the second thing that we need to do is go to the settings.py
and then navigate to the ITEM_PIPELINES

01:13.740 --> 01:26.130
which are going to be here, uncomment them
and then write the for example "MongoDBPipeline" which

01:26.130 --> 01:33.350
will actually go to the pipelines.py
and then assign class MongoDBPipeline in a moment or so.

01:33.600 --> 01:39.530
And the second thing that we need to do
is to assign different MongoDB attributes.

01:39.540 --> 01:47.450
One of them is going to be "MONGODB_SERVER",
which is going to be a "localhost".

01:47.460 --> 01:57.210
The second thing that we need to assign is
the port which is going to be "27017".

01:57.210 --> 02:03.900
Third thing is "MONGODB_DB" or the name of the database
which is going to be, let's call it "books".

02:04.400 --> 02:11.550
And the fourth or the last thing that  we need to do
is assign "MONGODB_COLLECTION", name of the collection

02:11.590 --> 02:18.360
is going to be for example "products",
collection once again once I refer to the collection that pretty

02:18.360 --> 02:24.210
much means table in the MySQL.
Save it, then go back

02:24.240 --> 02:27.020
and then finally go to the Pipelines

02:27.250 --> 02:34.090
Here we can delete this
and then type in "from pymongo"

02:34.420 --> 02:37.900
we will "import MongoClient".

02:37.980 --> 02:46.170
Second thing that we need to do is import Scrapy settings.
To do that from we can type in "from scrapy.conf import settings"

02:46.570 --> 02:57.810
Then assign "class" which is going to be
"MongoDBPipeline" this is same class as here.

02:58.500 --> 03:04.320
Since it's going to go to the pipelines.py
and then MongoDBPipeline.

03:04.320 --> 03:11.940
And then in the attributes, or parameters,
we can assign it to be an "object".

03:12.300 --> 03:16.530
We will initialize different parameters from the settings.py.

03:16.530 --> 03:20.620
"self" is the parameter.

03:20.670 --> 03:27.670
The first thing is we can assign the "connection"
which is going to be "MongoClient" so it will inherit

03:27.670 --> 03:40.400
from the pymongo, and here we will go to the "settings['MONGODB_SERVER']",
which is going to go to the settings.py

03:40.400 --> 03:47.300
and then go to the MONGODB_SERVER
which is going to be set to the localhost.

03:47.300 --> 03:51.710
Second thing that needs to be done is once again "settings".

03:56.510 --> 03:56.860
Okay.

03:56.890 --> 03:58.600
This is more like it.

03:58.930 --> 04:08.350
And then "MONGODB_PORT", which is going to be this number
and let's see what else needs

04:08.350 --> 04:08.990
to be done.

04:09.040 --> 04:15.000
Then the second thing that needs to be done here is
to assign database which is going to go to the settings

04:15.550 --> 04:22.360
and it's going to inherit from the...
or it will be named books, so to do that we can type in

04:22.360 --> 04:30.960
"db = connection", and then let's see
in the square brackets we can type in

04:32.020 --> 04:33.260
"settings['MONGODB_DB']"

04:37.470 --> 04:38.430
That's right.

04:38.750 --> 04:39.380
OK.

04:39.390 --> 04:47.670
And then finally "self.collection",
self is going to be necessary because we are going to write in a

04:47.670 --> 04:55.690
moment or so another function
and "self.collection = db[]".

04:56.030 --> 05:02.930
Let's see the settings, so "settings['MONGODB_COLLECTION']".

05:03.000 --> 05:10.610
Finally we will assign another function
which is going to be "process_item", and that function

05:10.610 --> 05:12.030
will have 

05:12.060 --> 05:21.320
"self, item, spider", this is also
a well known function that we already covered.

05:22.360 --> 05:32.920
And so we type in "self.collection.insert(dict(item))"

05:32.920 --> 05:37.660
and then close once again parenthesis.

05:37.680 --> 05:41.970
And then finally return item. We will save it and

05:42.230 --> 05:46.890
let's see if there is some mixture of tabs and spaces somewhere.

05:47.060 --> 05:56.430
There is not, so pretty much this line is going to
insert the data into the Mongo database, pretty self-explanatory.

05:56.430 --> 05:58.480
And if we run it.

05:58.500 --> 06:00.840
So go back to the root directory.

06:01.370 --> 06:04.760
Open in Terminal, let's maximize it.

06:04.820 --> 06:06.020
"scrapy crawl books".

06:06.060 --> 06:13.020
And we don't assign any other attributes
and stuff like that to get this written to the Mongo database.

06:13.020 --> 06:16.170
So let's hit Enter and let's see.

06:16.200 --> 06:17.580
So 20 or so.

06:17.640 --> 06:20.210
Yeah exactly 20 books are scraped.

06:20.580 --> 06:24.120
And let's go to the "mongo" and let's see.

06:24.160 --> 06:29.000
"show dbs", I haven't created any other database.

06:29.010 --> 06:34.200
And as you can see books database from the settings.py is created.

06:34.200 --> 06:44.250
So if we go in and then hit "use books",
and then "show collections", we should see

06:44.250 --> 06:47.250
the products listed.

06:47.310 --> 06:47.900
Perfect.

06:47.940 --> 06:54.220
And if we type in right now, let's see "db.collection"

06:57.110 --> 07:02.980
not collections, sorry, "products.find();"

07:03.130 --> 07:03.860
Perfect.

07:04.140 --> 07:11.170
And if we once again go and prettify it,
for example, perfect this is more readable.

07:11.190 --> 07:16.280
And as you can see we get written our data into the database.

07:16.350 --> 07:24.330
So if we want to get another batch,
we can just pretty much run the Spider as it is or just close it

07:24.990 --> 07:27.430
and then crawl it once again.

07:27.600 --> 07:32.780
And the way that we test it out is, let's see.

07:32.910 --> 07:34.470
So "show collections".

07:34.470 --> 07:35.400
Sorry.

07:35.550 --> 07:46.110
"use books"

07:46.110 --> 07:49.830
"db.products.find().pretty();"

07:50.100 --> 08:03.950
And here if we delete it or let's see
it should be somewhere around 40 or so books and here if we type in

08:04.190 --> 08:15.490
let's see "db" and if my memory is correct
then we can type in "db.products"

08:15.500 --> 08:21.180
".remove({});"

08:21.230 --> 08:28.970
We will see that there should be removed 40 products
because we already scrape the site two times, or

08:28.970 --> 08:37.190
the home page two times.
Perfect, so there are 40 removed products or really nodes in this case,

08:37.190 --> 08:37.810
such as this one.

08:38.000 --> 08:40.110
And that will be pretty much it for this video.

08:40.140 --> 08:41.050
And thanks for watching!
