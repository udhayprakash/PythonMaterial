WEBVTT

00:00:00.667 --> 00:00:05.092
Hi there! So today, we are going to talk
a bit more about Scrapy Architecture.


00:00:05.739 --> 00:00:09.121
When I say "Architecture", I mean
the overall layout of a Scrapy project.


00:00:09.650 --> 00:00:15.094
And what each field represents and how we can
finally use them in pair with your spider code.


00:00:15.467 --> 00:00:20.451
Let's start with the root directory which
contains the project folder, which this one.


00:00:20.811 --> 00:00:22.811
And then the configuration file.


00:00:23.296 --> 00:00:27.626
This file, Scrapy needs it to direct
it to where all project lies.


00:00:27.906 --> 00:00:33.454
The config file needs to be in the root directory,
so the code can be executed correctly.


00:00:34.477 --> 00:00:37.506
If we go into this file, you'll see
that it's pretty straight forward.


00:00:38.073 --> 00:00:42.648
So here we have two tabs: the first one is [settings],
and the second one is [deploy].


00:00:43.362 --> 00:00:50.501
The settings tab is going to pretty much inherit or
go to our project folder and go into settings.py


00:00:51.058 --> 00:00:55.596
And deploy tab is going to have the
url which is going to be localhost...


00:00:55.871 --> 00:00:59.805
...and "project" variable which is going
to direct to the project folder.


00:01:00.300 --> 00:01:06.157
Speaking of the project folder, let's actually go in,
and the first file, for example, is __init__.py


00:01:06.513 --> 00:01:12.446
This file is, of course, used to mark directories
on disk as Python package directories.


00:01:12.471 --> 00:01:15.291
The next one is items.py


00:01:15.864 --> 00:01:22.519
Now, you can pretty much "return" or "yield",
as we did it here in our Spider code.


00:01:22.819 --> 00:01:28.042
So this is the spider that is the
basic one from the third video.


00:01:28.282 --> 00:01:35.375
And, pretty much, once again you can yield
straight items from the spider code like we did here.


00:01:35.891 --> 00:01:43.996
Or you can define it in items.py, for example, and you
have more than a few good reason why you should do that.


00:01:44.364 --> 00:01:49.276
scrapy.Item are objects


00:01:50.376 --> 00:01:57.077
Well, the bottom line is that they are
simple containers used to collect the scraped data.


00:01:57.517 --> 00:02:00.786
And they provide an API similar to dictionaries...


00:02:01.906 --> 00:02:07.553
...with a convenient syntax
for declaring available fields.


00:02:07.943 --> 00:02:12.854
Now why would you, once again, want
to use data points defined in items?


00:02:13.284 --> 00:02:20.105
For example, if you want to get the data
in a csv file in some specific order...


00:02:20.105 --> 00:02:25.928
...for example the first column always has to
be H1 Tag and the second one has to be Tags,


00:02:26.531 --> 00:02:32.342
then defining items here and, kind of,
merging them with the Spider code...


00:02:33.180 --> 00:02:38.150
...should be an available option and
it will be the solution to the issue.


00:02:38.944 --> 00:02:45.952
For example, serialization can be customized and
you can more easily find memory leaks if they exist.


00:02:46.212 --> 00:02:52.715
And you will have more readable output of the scraped data
in Scrapy logs as you will see in a few minutes.


00:02:53.315 --> 00:02:55.402
So let's actually define our items.


00:02:55.582 --> 00:03:00.422
Currently we have just two items,
we can delete this "yield"...


00:03:01.040 --> 00:03:09.618
and "pass", we can also delete that
and let's define them as scrapy.Field()


00:03:09.778 --> 00:03:12.039
Save, then go to the spider code.


00:03:12.309 --> 00:03:18.052
What we need to do is to import two modules,
the first one is going to be ItemLoader.


00:03:18.417 --> 00:03:24.487
And it is going to be a tool for pretty much merging items
that pipe with the spider code or hold that pipe.


00:03:25.046 --> 00:03:32.756
So let's do that. So 
from scrapy.loader import ItemLoader


00:03:32.836 --> 00:03:36.975
Let's actually do the same thing
for Scrapy Spider.


00:03:36.975 --> 00:03:47.172
So from scrapy import Spider
and finally let's use single quotes instead of double.


00:03:48.185 --> 00:03:50.968
And the last module is
going to be the local one.


00:03:50.968 --> 00:03:58.588
So we are just going to go into items.py file
in our project directory and fetch this class...


00:03:58.588 --> 00:04:06.251
...and then this class will automatically fetch
these two tags defined here as Scrapy fields.


00:04:07.027 --> 00:04:14.372
So to do that, you should type in
from quotes_spider which is the name of the project...


00:04:15.385 --> 00:04:20.719
and then let's see .items
which will go to the items.py


00:04:21.357 --> 00:04:25.907
We will import the class which is
going to be QuoteSpiderItem


00:04:27.341 --> 00:04:35.773
And then in the parse function, we need to define the
first item loader; let's define it as a variable named "l"


00:04:36.154 --> 00:04:43.383
So type in ItemLoader
"item" is going to be pretty much this class.


00:04:43.497 --> 00:04:48.878
This was defined in items.py and then response
is just going to equal to the response...


00:04:49.152 --> 00:04:54.047
...meaning pretty much, this page, aka
Home Page of quotes.toscrape.com


00:04:56.136 --> 00:04:58.903
Let's actually, kind of, load them.


00:04:59.146 --> 00:05:13.613
To do that, you should type in l.add_value()
and then let's copy and paste this code.


00:05:14.291 --> 00:05:20.615
And here we can type in "h1_tag", as
you can see, and then "tags" here.


00:05:20.915 --> 00:05:31.737
Finally, we can just "return" them like we did or "yield" them
previously by typing in l.load_item()


00:05:32.305 --> 00:05:38.642
Save it, go to the Terminal, type in scrapy crawl quotes
Now let's see if it will work.


00:05:41.213 --> 00:05:43.213
As you can see, it works.


00:05:43.890 --> 00:05:50.825
So previously, we didn't have
pre-defined items so to speak...


00:05:51.259 --> 00:06:00.729
...meaning that every sub-item will be in or
each item will be declared or using a new line.


00:06:01.000 --> 00:06:08.262
For example, if you have a task to scrape 50+
items from some specific page...


00:06:08.818 --> 00:06:15.974
...then, because of these items, once you are using items.py
and merging again spider with the items...


00:06:16.504 --> 00:06:21.209
...you will have them in an ordered list
or in ordered dictionary.


00:06:21.336 --> 00:06:31.539
Which is going to be great for quickly navigating
and then debugging items if, of course, necessary.


00:06:32.563 --> 00:06:35.090
Let's go back to our spider code.


00:06:36.003 --> 00:06:39.268
And let's see what else
we need to figure out.


00:06:40.185 --> 00:06:43.670
The next file is going to be called pipelines.py


00:06:43.943 --> 00:06:49.959
pipelines.py is going to be used when receiving
an item and performing an action over it.


00:06:50.322 --> 00:06:58.264
Also deciding if the item should continue through pipeline
or be dropped and no longer processed.


00:06:58.304 --> 00:07:02.633
For example, let's say you have a task
to scrape some e-commerce site...


00:07:02.633 --> 00:07:08.563
...and the price included does not include, for
example, your local taxes or something like that.


00:07:08.966 --> 00:07:19.812
So then you would isolate some specific item names such as
"price" and multiply it by 20 or 50, or whatever percent...


00:07:20.466 --> 00:07:24.050
...and then return item
like we did or like we see here.


00:07:24.901 --> 00:07:29.525
Let's actually make an example
and go to our spider code.


00:07:29.885 --> 00:07:33.718
h1_tag, as you can see the
output is in the form of a list.


00:07:33.789 --> 00:07:39.963
The first and only item in the list is
in the form of Unicode...


00:07:39.963 --> 00:07:42.577
...and it reads "Quotes to Scrape".


00:07:43.222 --> 00:07:47.713
So, let's actually go to the
pipelines.by and find, for example...


00:07:47.713 --> 00:07:54.991
...if statements, so "if item" and then
the dictionary key is going to be "h1_tag".


00:07:56.096 --> 00:07:58.591
So, if item is h1_tag


00:07:58.672 --> 00:08:03.064
or the dictionary key of the item
is h1_tag, we will process them...


00:08:04.270 --> 00:08:13.651
...by using, or pretty much
defining it as an upper case really.


00:08:14.035 --> 00:08:18.306
So this will be all upper case
as you will see in a moment.


00:08:19.184 --> 00:08:23.430
And then you can just return item,
this should work, so let's try it out.


00:08:23.830 --> 00:08:26.827
And most likely it won't,
okay so it won't.


00:08:27.155 --> 00:08:34.049
The reason is because we need to go to our last file
which is called settings.py and then find here, pipelines.


00:08:34.227 --> 00:08:37.718
Here, as you can see here,
we have item pipelines.


00:08:38.597 --> 00:08:44.909
And by uncommenting this and going in
and copying and pasting this specific class.


00:08:45.378 --> 00:08:49.549
Saving it and then running
it once again as you'll see.


00:08:49.975 --> 00:08:54.996
Here we have again
some error with processing.


00:08:54.996 --> 00:08:58.168
So let's see what the issue is.


00:09:01.312 --> 00:09:11.258
Okay, so h1_tag and if is then...
okay, so I already know the reason.


00:08:40.432 --> 00:09:16.508
The reason is because we are
going to use upper case in our list.


00:09:16.533 --> 00:09:20.112
And that's not going to work, so
we’ll have to use with slicing.


00:09:20.334 --> 00:09:27.291
So let's run it once again and as you can see, it's in
the form of a Unicode right now and it's all upper case.


00:09:28.046 --> 00:09:29.663
So pretty good.


00:09:29.823 --> 00:09:38.452
To get back to the settings.py let's see, this
is our last file that we want to cover today.


00:09:39.547 --> 00:09:47.385
Here we already covered some of the settings such
as, just previously mentioned, ITEM PIPELINES.


00:09:47.633 --> 00:09:51.942
For example, ROBOTSTXT_OBEY is set to False.


00:09:52.609 --> 00:10:06.528
For example, some sites are not going to declare,
or really you won't be able to see your data...


00:10:06.780 --> 00:10:10.961
...points or really just
anything from the site...


00:10:11.480 --> 00:10:20.110
...if you do not use a USER_AGENT - so just bear that in mind;
if a site, for example, uses some status code as...


00:10:20.689 --> 00:10:30.093
...503 or 504, something like that, nost likely you will
be, sort of, detected without having any user agents.


00:10:30.446 --> 00:10:32.340
And so you'll have to define it here.


00:10:33.116 --> 00:10:40.774
The most important one, for me at least
is called DOWNLOAD_DELAY.


00:10:41.874 --> 00:10:48.864
If you saved settings.py like this, you will
download each request by 3 seconds.


00:10:49.580 --> 00:10:54.422
And you can use this to be
"a bit more friendlier to sites".


00:10:55.967 --> 00:11:02.418
And also you can, for example, each and every setting
you can define it here once you run the spider.


00:11:02.538 --> 00:11:06.257
So you can define it as
-s which stands for settings.


00:11:06.709 --> 00:11:11.841
Then all upper case, for example
DOWNLOAD_DELAY and then set it as 5 or so.


00:11:11.841 --> 00:11:14.536
So this will delay, again,
each request by 5 seconds.


00:11:16.688 --> 00:11:19.231
There are bunch of settings to be honest.


00:11:19.388 --> 00:11:26.311
Cookies for example, for some reason,
they might cause some issues.


00:11:27.291 --> 00:11:32.277
...with you, so you can just
define them or use them.


00:11:32.803 --> 00:11:37.445
Also there are DEFAULT_REQUEST_HEADERS
something similar to the USER_AGENT.


00:11:37.445 --> 00:11:43.116
Some sites don't allow you to view
their data without request headers.


00:11:44.003 --> 00:11:46.989
Also there are a lot of
extensions and middleware.


00:11:47.447 --> 00:11:49.607
ITEMS_PIPELINES, again, we've covered that.


00:11:50.169 --> 00:11:53.548
These are for auto-throttling.


00:11:54.108 --> 00:11:59.610
And this is for the HTTP cache and
the settings corresponding to it.


00:12:00.627 --> 00:12:05.524
And that will be it pretty much. So once again
you can, pretty much set it here in settings.


00:12:05.881 --> 00:12:10.173
Or you can go in once you run it
and then define, for example...


00:12:11.054 --> 00:12:18.413
...of course, all upper case, USER_AGENT and then define
it as Mozilla and then the version number, etc.


00:12:19.629 --> 00:12:27.830
And that will be it for the fifth video. We will get
back to building something in the very next one.


00:12:28.010 --> 00:12:30.262
And I'll see you there. Talk soon!

