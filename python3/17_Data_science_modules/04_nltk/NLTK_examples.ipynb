{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.  NLTK program to split the text sentence/paragraph into a list of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original string:\n",
      "\n",
      "Joe waited for the train. The train was late. \n",
      "Mary and Samantha took the bus. \n",
      "I looked for Mary and Samantha at the bus station.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "Joe waited for the train. The train was late. \n",
    "Mary and Samantha took the bus. \n",
    "I looked for Mary and Samantha at the bus station.\n",
    "\"\"\"\n",
    "print(\"\\nOriginal string:\")\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentence-tokenized copy in a list:\n",
      "['\\nJoe waited for the train.', 'The train was late.', 'Mary and Samantha took the bus.', 'I looked for Mary and Samantha at the bus station.']\n",
      "\n",
      "Read the list:\n",
      "\n",
      "Joe waited for the train.\n",
      "The train was late.\n",
      "Mary and Samantha took the bus.\n",
      "I looked for Mary and Samantha at the bus station.\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "token_text = sent_tokenize(text)\n",
    "print(\"\\nSentence-tokenized copy in a list:\")\n",
    "print(token_text)\n",
    "\n",
    "print(\"\\nRead the list:\")\n",
    "for s in token_text:\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. program to tokenize sentences in languages other than English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original string:\n",
      "\n",
      "NLTK ist Open Source Software. Der Quellcode wird unter den Bedingungen der Apache License Version 2.0 vertrieben.  \n",
      "Die Dokumentation wird unter den Bedingungen der Creative Commons-Lizenz Namensnennung - Nicht kommerziell - Keine \n",
      "abgeleiteten Werke 3.0 in den Vereinigten Staaten verteilt.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "NLTK ist Open Source Software. Der Quellcode wird unter den Bedingungen der Apache License Version 2.0 vertrieben.  \n",
    "Die Dokumentation wird unter den Bedingungen der Creative Commons-Lizenz Namensnennung - Nicht kommerziell - Keine \n",
    "abgeleiteten Werke 3.0 in den Vereinigten Staaten verteilt.\n",
    "\"\"\"\n",
    "print(\"\\nOriginal string:\")\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentence-tokenized copy in a list:\n",
      "['\\nNLTK ist Open Source Software.', 'Der Quellcode wird unter den Bedingungen der Apache License Version 2.0 vertrieben.', 'Die Dokumentation wird unter den Bedingungen der Creative Commons-Lizenz Namensnennung - Nicht kommerziell - Keine \\nabgeleiteten Werke 3.0 in den Vereinigten Staaten verteilt.']\n",
      "\n",
      "Read the list:\n",
      "\n",
      "NLTK ist Open Source Software.\n",
      "Der Quellcode wird unter den Bedingungen der Apache License Version 2.0 vertrieben.\n",
      "Die Dokumentation wird unter den Bedingungen der Creative Commons-Lizenz Namensnennung - Nicht kommerziell - Keine \n",
      "abgeleiteten Werke 3.0 in den Vereinigten Staaten verteilt.\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "token_text = sent_tokenize(text, language=\"german\")\n",
    "print(\"\\nSentence-tokenized copy in a list:\")\n",
    "print(token_text)\n",
    "\n",
    "print(\"\\nRead the list:\")\n",
    "for s in token_text:\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.  program to create a list of words from a given string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original string:\n",
      "Joe waited for the train. The train was late. Mary and Samantha took the bus. I looked for Mary and Samantha at the bus station.\n",
      "\n",
      "List of words:\n",
      "['Joe', 'waited', 'for', 'the', 'train', '.', 'The', 'train', 'was', 'late', '.', 'Mary', 'and', 'Samantha', 'took', 'the', 'bus', '.', 'I', 'looked', 'for', 'Mary', 'and', 'Samantha', 'at', 'the', 'bus', 'station', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"Joe waited for the train. The train was late. Mary and Samantha took the bus. I looked for Mary and Samantha at the bus station.\"\n",
    "print(\"\\nOriginal string:\")\n",
    "print(text)\n",
    "print(\"\\nList of words:\")\n",
    "print(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original string:\n",
      "Joe waited for the train. The train was late. Mary and Samantha took the bus. I looked for Mary and Samantha at the bus station.\n",
      "\n",
      "List of words:\n",
      "['Joe', 'waited', 'for', 'the', 'train.', 'The', 'train', 'was', 'late.', 'Mary', 'and', 'Samantha', 'took', 'the', 'bus.', 'I', 'looked', 'for', 'Mary', 'and', 'Samantha', 'at', 'the', 'bus', 'station', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "text = \"Joe waited for the train. The train was late. Mary and Samantha took the bus. I looked for Mary and Samantha at the bus station.\"\n",
    "print(\"\\nOriginal string:\")\n",
    "print(text)\n",
    "print(\"\\nList of words:\")\n",
    "print(tokenizer.tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.  program to split all punctuation into separate tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original string:\n",
      "Reset your password if you just can't remember your old one.\n",
      "\n",
      "Split all punctuation into separate tokens:\n",
      "['Reset', 'your', 'password', 'if', 'you', 'just', 'can', \"'\", 't', 'remember', 'your', 'old', 'one', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "text = \"Reset your password if you just can't remember your old one.\"\n",
    "print(\"\\nOriginal string:\")\n",
    "print(text)\n",
    "result = WordPunctTokenizer().tokenize(text)\n",
    "print(\"\\nSplit all punctuation into separate tokens:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. program to tokenize words, sentence wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original string:\n",
      "Joe waited for the train. The train was late. Mary and Samantha took the bus. I looked for Mary and Samantha at the bus station.\n",
      "\n",
      "Tokenize words sentence wise:\n",
      "\n",
      "Read the list:\n",
      "['Joe', 'waited', 'for', 'the', 'train', '.']\n",
      "['The', 'train', 'was', 'late', '.']\n",
      "['Mary', 'and', 'Samantha', 'took', 'the', 'bus', '.']\n",
      "['I', 'looked', 'for', 'Mary', 'and', 'Samantha', 'at', 'the', 'bus', 'station', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "text = \"Joe waited for the train. The train was late. Mary and Samantha took the bus. I looked for Mary and Samantha at the bus station.\"\n",
    "print(\"\\nOriginal string:\")\n",
    "print(text)\n",
    "print(\"\\nTokenize words sentence wise:\")\n",
    "result = [word_tokenize(t) for t in sent_tokenize(text)]\n",
    "print(\"\\nRead the list:\")\n",
    "for s in result:\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. program to tokenize a twitter text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original Tweet:\n",
      "NoSQL introduction - w3resource http://bit.ly/1ngHC5F  #nosql #database #webdev\n",
      "\n",
      "Tokenize a twitter text:\n",
      "['NoSQL', 'introduction', '-', 'w3resource', 'http://bit.ly/1ngHC5F', '#nosql', '#database', '#webdev']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "tknzr = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "tweet_text = (\n",
    "    \"NoSQL introduction - w3resource http://bit.ly/1ngHC5F  #nosql #database #webdev\"\n",
    ")\n",
    "print(\"\\nOriginal Tweet:\")\n",
    "print(tweet_text)\n",
    "result = tknzr.tokenize(tweet_text)\n",
    "print(\"\\nTokenize a twitter text:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. program to remove Twitter username handles from a given twitter text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original Tweet:\n",
      "@abcd @pqrs NoSQL introduction - w3resource http://bit.ly/1ngHC5F  #nosql #database #webdev\n",
      "\n",
      "Tokenize a twitter text:\n",
      "['NoSQL', 'introduction', '-', 'w3resource', 'http://bit.ly/1ngHC5F', '#nosql', '#database', '#webdev']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "tknzr = TweetTokenizer(strip_handles=True)\n",
    "tweet_text = \"@abcd @pqrs NoSQL introduction - w3resource http://bit.ly/1ngHC5F  #nosql #database #webdev\"\n",
    "print(\"\\nOriginal Tweet:\")\n",
    "print(tweet_text)\n",
    "result = tknzr.tokenize(tweet_text)\n",
    "print(\"\\nTokenize a twitter text:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. program that will read a given text through each line and look for sentences. Print each sentence and divide two sentences with “==============”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original Tweet:\n",
      "\n",
      "Mr. Smith waited for the train. The train was late.\n",
      "Mary and Samantha took the bus. I looked for Mary and\n",
      "Samantha at the bus station.\n",
      "\n",
      "Mr. Smith waited for the train.\n",
      "==============\n",
      "The train was late.\n",
      "==============\n",
      "Mary and Samantha took the bus.\n",
      "==============\n",
      "I looked for Mary and\n",
      "Samantha at the bus station.\n"
     ]
    }
   ],
   "source": [
    "import nltk.data\n",
    "\n",
    "text = \"\"\"\n",
    "Mr. Smith waited for the train. The train was late.\n",
    "Mary and Samantha took the bus. I looked for Mary and\n",
    "Samantha at the bus station.\n",
    "\"\"\"\n",
    "print(\"\\nOriginal Tweet:\")\n",
    "print(text)\n",
    "sent_detector = nltk.data.load(\"tokenizers/punkt/english.pickle\")\n",
    "print(\"\\n==============\\n\".join(sent_detector.tokenize(text.strip())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original Tweet:\n",
      "\n",
      "Mr. Smith waited for the train. (The train was late.)\n",
      "Mary and Samantha took the bus. I looked for Mary and\n",
      "Samantha at the bus station [Sector-1].\n",
      "\n",
      "Mr. Smith waited for the train.\n",
      "==============\n",
      "(The train was late.)\n",
      "==============\n",
      "Mary and Samantha took the bus.\n",
      "==============\n",
      "I looked for Mary and\n",
      "Samantha at the bus station [Sector-1].\n"
     ]
    }
   ],
   "source": [
    "import nltk.data\n",
    "\n",
    "text = \"\"\"\n",
    "Mr. Smith waited for the train. (The train was late.)\n",
    "Mary and Samantha took the bus. I looked for Mary and\n",
    "Samantha at the bus station [Sector-1].\n",
    "\"\"\"\n",
    "print(\"\\nOriginal Tweet:\")\n",
    "print(text)\n",
    "sent_detector = nltk.data.load(\"tokenizers/punkt/english.pickle\")\n",
    "print(\"\\n==============\\n\".join(sent_detector.tokenize(text.strip())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. program to find parenthesized expressions in a given string and divides the string into a sequence of substrings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original Tweet:\n",
      "(a b (c d)) e f (g)\n",
      "['(a b (c d))', 'e', 'f', '(g)']\n",
      "\n",
      "Original Tweet:\n",
      "(a b) (c d) e (f g)\n",
      "['(a b)', '(c d)', 'e', '(f g)']\n",
      "\n",
      "Original Tweet:\n",
      "[(a b (c d)) e f (g)]\n",
      "['[', '(a b (c d))', 'e', 'f', '(g)', ']']\n",
      "\n",
      "Original Tweet:\n",
      "{a b {c d}} e f {g}\n",
      "['{a b {c d}} e f {g}']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import SExprTokenizer\n",
    "\n",
    "text = \"(a b (c d)) e f (g)\"\n",
    "print(\"\\nOriginal Tweet:\")\n",
    "print(text)\n",
    "print(SExprTokenizer().tokenize(text))\n",
    "\n",
    "text = \"(a b) (c d) e (f g)\"\n",
    "print(\"\\nOriginal Tweet:\")\n",
    "print(text)\n",
    "print(SExprTokenizer().tokenize(text))\n",
    "\n",
    "text = \"[(a b (c d)) e f (g)]\"\n",
    "print(\"\\nOriginal Tweet:\")\n",
    "print(text)\n",
    "print(SExprTokenizer().tokenize(text))\n",
    "\n",
    "text = \"{a b {c d}} e f {g}\"\n",
    "print(\"\\nOriginal Tweet:\")\n",
    "print(text)\n",
    "print(SExprTokenizer().tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. program to list down all the corpus names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Available corpus names:\n",
      "['_LazyModule__lazymodule_globals', '_LazyModule__lazymodule_import', '_LazyModule__lazymodule_init', '_LazyModule__lazymodule_loaded', '_LazyModule__lazymodule_locals', '_LazyModule__lazymodule_name', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__name__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__']\n"
     ]
    }
   ],
   "source": [
    "import nltk.corpus\n",
    "\n",
    "print(\"\\nAvailable corpus names:\")\n",
    "print(dir(nltk.corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
